\documentclass[11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[myheadings]{fullpage}
\DeclareUnicodeCharacter{0301}{\hspace{-1ex}\'{ }}

\usepackage{silence}

% Package for headers 
\usepackage{fancyhdr}
\usepackage{lastpage}

% For figures and stuff
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}

% Change for different font sizes and families
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}

% Maths
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Code
\usepackage{minted}
\usepackage{adjustbox}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwProg{While}{while}{}{}
\SetKwProg{For}{for}{}{}
\SetKwProg{Function}{function}{}{}

% Bibliography
\usepackage{biblatex} 
\addbibresource{../references.bib}

%Glossary
\usepackage[acronym]{glossaries}
\makeglossaries
\input{../glossary}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{indentfirst}

%Tabular
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{enumitem}

%Function draw

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{tikz}
\usepackage{tikz-uml}
\usepackage{amsmath}
\usepackage{pgfmath}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\usetikzlibrary{fit, backgrounds, calc}

\usepackage{afterpage}

% Show mini ToCs automatically at the beginning of each chapter
\usepackage{minitoc}
\setcounter{tocdepth}{0}        % Only show chapters in the main ToC
\setcounter{minitocdepth}{2}    % Show sections and subsections in mini ToCs
\dominitoc                      % Enable mini ToCs
\WarningFilter{minitoc(hints)}{W0024}

\definecolor{terminalbg}{RGB}{0,0,0}
\definecolor{terminalfg}{RGB}{255, 255, 255} 

\newminted[console]{bash}{
  bgcolor=terminalbg,
  fontfamily=tt,
  fontsize=\small,
  linenos=false,
  framesep=100mm,
  rulecolor=\color{white},
  baselinestretch=1,
  breaklines=true
}


\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=1.5cm,left=2cm,right=2cm,marginparwidth=1.5cm]{geometry}
% Command to draw a horizontal line
\newcommand{\separator}{%
    \begin{center}
      {\Large$\ast$~$\ast$~$\ast$}
    \end{center}
}


\setlength{\parskip}{10pt}
\setlength{\parindent}{1cm}

\pagestyle{fancy}
\fancyhf{}

% Header and footer information
\setlength\headheight{15pt}
\author{Mathys VINATIER}
%\fancyhead[L]{IPSA} 
\fancyhead[R]{Mathys VINATIER}
\fancyfoot[R]{\thepage}
 \setlength {\marginparwidth }{2cm}
\begin{document}


\date{}

\title{ 
		\HRule{2pt} \\
		\LARGE \textbf{ Report on Kalman Filter applied on Neural Network for Finance } 
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}\\
		Mathys VINATIER  - \href{https://github.com/MathysVinatier/17_SNU_Lab/blob/main/}{GitHub Project page}\\
		Supervisor:      \\
		  Pr Kim Tae-Wan
      }

\maketitle

\tableofcontents
\blankpage

\newpage

\setcounter{chapter}{9}
\chapter{Week 10}
\minitoc

\newpage
\blankpage

\newpage

\section{Reminder on Greedy Policy and QLearning}


Q-Learning is a \textbf{model-free, off-policy, value-based reinforcement learning algorithm} that trains an action-value function (Q-function) to find the optimal policy indirectly. The Q-function estimates the expected cumulative reward of taking a certain action in a given state and following the best policy thereafter.

\begin{itemize}
    \item \textbf{Q-table:} Internally, Q-Learning uses a Q-table storing values for each state-action pair. Initially, all values are zero, and the table is updated iteratively during training.
    \begin{center}
        \begin{tikzpicture}[
            scale=0.7,
            node distance=1.1cm and 2.2cm,
            box/.style={draw, rounded corners, minimum width=2.1cm, minimum height=0.7cm, align=center, font=\small, fill=blue!10},
            arrow/.style={-{Latex[length=2mm]}, thick}
        ]
        % Nodes
        \node[box] (state) {State};
        \node[box, right=of state, yshift=-0.8cm] (qtable) {Q-Table};
        \node[box, below=of state] (action) {Action};
        \node[box, right=of qtable] (qvalue) {Q-Value};
        % Arrows
        \draw[arrow] (state) -- (qtable);
        \draw[arrow] (action) -- (qtable);
        \draw[arrow] (qtable) -- (qvalue);
        % Labels
        \node[left=1mm of qtable] {\scriptsize{Input}};
        \node[right=1mm of qtable, yshift=3mm] {\scriptsize{Output}};   
        \end{tikzpicture}
    \end{center}
    \item \textbf{Epsilon-Greedy Strategy:} To balance exploration and exploitation, actions are chosen using an epsilon-greedy policy:
    \[
        \pi^*(s) = \arg\max_{a} Q^*(s, a)
    \]
        \begin{itemize}
            \item With probability \(1 - \epsilon\), exploit by selecting the action with the highest Q-value.
            \item With probability \(\epsilon\), explore by selecting a random action.
            \item \(\epsilon\) decays over time to shift from exploration to exploitation.
        \end{itemize}
    \begin{center}
        \begin{tikzpicture}[
        scale=0.7,
        node distance=0.5cm and 3cm,
        box/.style={draw, rounded corners, minimum width=3cm, minimum height=0.7cm, align=center, fill=blue!10, font=\small},
        arrow/.style={-{Latex[length=2mm]}, thick}
        ]
        % Nodes
        \node[box] (policy) {\textbf{$\epsilon$-greedy policy}};
        \node[box, above right=of policy] (exploit) {Exploitation\\(greedy action)};
        \node[box, below right=of policy] (explore) {Exploration\\(random action)};
        % Arrows
        \draw[arrow] (policy.east) -- node[above, pos=0.5, rotate=30] {$1-\epsilon$} (exploit.west);
        \draw[arrow] (policy.east) -- node[below, pos=0.5] {$\epsilon$} (explore.west);
        \end{tikzpicture}
    \end{center}
    \item \textbf{TD Update:} At each step, the Q-value for the current state-action pair \(Q(s_t, a_t)\) is updated based on the immediate reward \(r_{t+1}\) plus the discounted maximum Q-value of the next state \(s_{t+1}\):
        \[
        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)\right]
        \]
    \item \textbf{Off-policy Learning:} The policy used to select actions (epsilon-greedy) differs from the policy used to compute the target update (greedy). This distinction allows Q-Learning to learn optimal policies even when exploring randomly.
\end{itemize}

After enough training episodes, the Q-table converges to the optimal Q-function, which directly yields the optimal policy by choosing actions with the highest Q-values in each state.


\newpage

\section{Implementation Progress and Experiments}

\indent This week was dedicated to the initial implementation and experimentation phase of the project, focusing primarily on the Proximal Policy Optimization (PPO) basis algorithms. Using the Hugging Face classes, we developed a prototype with greedy policy agent and integrated it into a custom trading environment inspired by the concepts discussed in previous weeks.

\vspace{0.3cm}

\subsection{Environment Setup}

\indent The custom environment was designed to simulate stock market trading dynamics with the following key elements :

\begin{itemize}
\item \textbf{State representation} including historical price data, technical indicators (moving averages, RSI) and Kalman filtered signals.
\item \textbf{Action space} consisting of discrete trading actions: Buy, Hold or Sell.
\item \textbf{Reward structure} based on portfolio value change and risk-adjusted metrics to incentivize both profit and stability.
\item \textbf{Episode design} with fixed-length trading periods to allow episodic evaluation of agent performance.
\end{itemize}

\noindent The environment was implemented adhering to OpenAI Gym interfaces, allowing smooth integration with standard RL training pipelines. Here is the UML of the created environement :

\vfill

\begin{center}
\begin{tikzpicture}
% TradingEnv class
\umlclass[width=.4\linewidth, x=0, y=0]{TradingEnv}{
    - df : DataFrame \\
    - n\_steps : int \\
    - current\_step : int \\
    - initial\_balance : float \\
    - balance : float \\
    - position : int \\
    - last\_action : int \\
    - observation\_space : Box \\
    - action\_space : Discrete
}{
    + \_\_init\_\_(df) \\
    + \_get\_obs() \\
    + sample\_valid\_action() \\
    + get\_valid\_actions() \\
    + step(action) \\
    + set\_data(df) \\
    + reset()
}

  % QLearning class
  \umlclass[x=8, y=0]{QLearning}{
    - env : TradingEnv \\
    - log : bool \\
    - state\_space : int \\
    - action\_space : int
  }{
    + \_\_init\_\_(env, log=True) \\
    + train(...) \\
    + greedy\_policy(Qtable, state) \\
    + epsilon\_greedy\_policy(Qtable, state\_idx, epsilon) \\
    + discretize\_state(state, bins) \\
    + state\_to\_index(discrete\_state, bins) \\
    + initialize\_q\_table(bins) \\
    + split\_data(df, train\_size) \\
    + get\_actions\_and\_prices(Qtable, df, initial\_cash=100) \\
    + plot(df, Qtable)
  }

  % Association link
  \umlassoc[geometry=-|, arm1=1, arm2=1, anchor1=west, anchor2=east]{QLearning}{TradingEnv}
\end{tikzpicture}
\end{center}

\vfill

\newpage

\begin{algorithm}[H]
\caption{Trading Environment Behavior}
\KwIn{Environment $env$, number of episodes $N$, exploration rate $\epsilon$}
\KwOut{Episode value and rewards}

\For{$episode \leftarrow 1$ \KwTo $N$}{
    $s \leftarrow env.reset()$\;
    total\_reward $\leftarrow 0$\;
    
    \While{True}{
        Get valid actions $A_{valid} \leftarrow env.get\_valid\_actions()$\;
        
            Select $a \leftarrow$ random choice from $A_{valid}$\;
        Execute $(s', r, done, info) \leftarrow env.step(a)$\;
        total\_reward $\leftarrow$ total\_reward $+ r$\;
        
        Log $(episode, s, a, r, info["portfolio\_value"])$\;
        
        $s \leftarrow s'$\;
        
        \If{$done$}{break}
    }
    
    Print episode summary : total\_reward, final portfolio value\;
}
\end{algorithm}

\begin{algorithm}[H]
\caption{Q-Learning Training }
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}

\newpage

\indent In the context of Q-learning for trading environments, bins are used to discretize the continuous state space into a finite number of discrete states. This discretization is essential because the Q-table requires a finite and manageable number of states to store and update the expected rewards for each state-action pair. Continuous variables, such as stock prices or technical indicators, have infinite possible values, which makes it impractical to represent them directly in a Q-table. To address this, we divide the range of each continuous feature into a fixed number of intervals called bins. Each bin represents a segment of the feature's value range, allowing us to map any continuous observation into a discrete category. For implementation, we analyze the training dataset and create evenly spaced bins (10 bins) for each feature by computing linearly spaced intervals between the minimum and maximum values of that feature. Then, during training or evaluation, the observed continuous state is converted into a discrete state by determining which bin each feature value falls into. This discretized state is subsequently converted into a unique index to access and update the Q-table. The use of bins thus enables effective Q-learning in continuous state environments by simplifying the state representation while preserving essential information.

\vspace{0.3cm}

\subsection{Preliminary Results for Q-Learning Greedy Policy}

\indent Initial experiments aimed to evaluate the training behavior and reward progression of the Q-Learning agent following a greedy policy. Results were compared over two time horizons : a shorter training period of 100 episodes and an extended training of 1000 episodes.

\subsubsection{100 episodes}

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2010.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2011.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2012.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2013.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2014.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2015.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2016.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2017.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2018.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2019.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2020.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2021.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2022.png}
\vfill

\subsubsection{1000 episodes}

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2010.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2011.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2012.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2013.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2014.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2015.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2016.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2017.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2018.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2019.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2020.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2021.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2022.png}
\vfill


\begin{itemize}
\item Over 100 episodes, the cumulative rewards exhibited a modest upward trend, indicating initial learning progress, though with significant variability.
\item Extending to 1000 episodes resulted showed that the agent is not taking position anymore, a convergence of the hold position shouyld be evaluated
\item Value estimates became more consistent over time, though some fluctuations persisted due to the noisy environment dynamics (should add filtering or smoothing).
\item The results highlight the trade-off between exploration and exploitation inherent in the greedy approach and motivate future exploration of strategies with controlled exploration. Noticed that the agent is not learning from the path but more on the direct previous value.
\end{itemize}

\noindent These preliminary findings establish a baseline for Q-Learning performance and set the stage for comparisons with other reinforcement learning methods such as Deep Q-Networks (DQN).

\vspace{0.3cm}

\section{Challenges and Solutions}

\indent The Q-Learning experiments faced several challenges specific to the greedy policy and the trading environment :

\begin{itemize}
\item \textbf{Limited exploration due to greediness} : The strictly greedy policy limited the agent's ability to discover better actions, particularly early in training. Future work will include incorporating greedy or softmax action selection to balance exploration.
\item \textbf{Reward noise and sparsity} : The stochastic environment produced noisy and sparse rewards, complicating learning.
\item \textbf{Hyperparameter sensitivity} : Learning rate and discount factor tuning were critical, especially over longer training horizons. We will use optuna to get better hyperparameter in the future on more complexe models.
\item \textbf{Computational efficiency} : Although less demanding than deep RL methods, Q-Learning with large state spaces required careful management of updates. We leveraged experience replay buffers and batch updates to accelerate convergence.
\end{itemize}

\section{Next Steps}

\indent Building on the current progress with Q-Learning and the greedy policy, the immediate next steps focus on advancing the model complexity and preparing for PPO :

\begin{itemize}
\item Designing and integrating a deep neural network architecture to implement Deep Q-Learning (DQN), enabling function approximation for larger and continuous state spaces.
\item Conducting experiments to compare the performance of DQN against the baseline Q-Learning greedy policy.
\item Following the DQN implementation, proceeding with the development and fine-tuning of the PPO agent, including enhanced reward shaping and hyperparameter optimization.
\item Expanding the trading environment to model more realistic market conditions such as transaction costs, slippage, and possibly varying liquidity.
\item Performing ablation studies to isolate the effects of PPO-specific components like clipping, value function loss weighting, and entropy regularization.
\item Enhancing visualization tools to track detailed performance metrics such as Sharpe ratio, maximum drawdown, and other risk-adjusted return measures.
\end{itemize}

\noindent These steps will establish a robust foundation for evaluating advanced reinforcement learning algorithms in the trading domain and inform subsequent research and publication efforts.

\newpage

\chapter{Week 11}
\minitoc

\newpage
\blankpage

\newpage

\section{Deep Q-Learning}

\subsection{Problem Formulation}
\indent We cast single-asset trading as a finite-horizon Markov Decision Process (MDP) on time-indexed observations $\{o_t\}_{t=1}^T$. At each discrete time step $t$, the agent observes a state $s_t \in \mathcal{S}$ (Price, Close, High, Low,Open and Volume), takes an action $a_t \in \mathcal{A}$(Buy, Old or Sell), receives a reward $r_t \in \mathbb{R}$.

\paragraph{State.}
To incorporate temporal context, we define
\[
s_t = \big[\phi(o_{t-w+1}),\,\ldots,\,\phi(o_{t})\big] \in \mathbb{R}^{w\times d},
\]
where $w$ is a rolling window length and $\phi(\cdot)$ is a feature map including (but not limited to): log-returns, rolling z-scores, realized volatility estimates, microstructure features (e.g., imbalance), and technical indicators. To avoid lookahead bias, all rolling statistics are computed using past data only and fit on the training split.

\paragraph{Action Space.}
We consider a discrete policy with position control and an optional \emph{hold} action:
\[
\mathcal{A}=\{-K,\ldots,-1,0,1,\ldots,K\},
\]
where $a_t$ represents the target position (short to long) in normalized units subject to a position limit $|a_t|\leq K$. An alternative is the set $\{\textsc{Short}, \textsc{Flat}, \textsc{Long}\}$ with an additional \textsc{Hold} that preserves the prior position.

\paragraph{Reward.}
Let $P_t$ be the execution price proxy, $\Delta P_{t+1}=P_{t+1}-P_t$, and $\Delta a_t=a_t-a_{t-1}$. A trading-aware reward that accounts for P\&L, costs, and risk is
\begin{equation}
\label{eq:reward}
r_t \;=\; a_t \cdot \Delta P_{t+1}
\;-\; c\,|\Delta a_t|
\;-\; \lambda_{\mathrm{risk}}\,\hat{\sigma}_{t}^2
\;-\; \lambda_{\mathrm{dd}}\,\max(0, \mathrm{DD}_t - \mathrm{DD}_{\max}),
\end{equation}
where $c$ is per-unit transaction/slippage cost, $\hat{\sigma}_t^2$ is an ex-ante volatility estimate, and $\mathrm{DD}_t$ is running drawdown. The last term softly penalizes breach of a drawdown budget $\mathrm{DD}_{\max}$.

\subsection{Deep Q-Network (DQN)}
DQN approximates the action-value function $Q^\star(s,a)$ with a neural network $Q_\theta(s,a)$ trained to minimize the temporal-difference (TD) error using a replay buffer $\mathcal{D}$ and a target network $Q_{\bar{\theta}}$.

\paragraph{Bellman Target.}
For terminal indicator $d_{t+1}\in\{0,1\}$,
\begin{align}
y_t^{\text{DQN}} &= r_t + \gamma (1-d_{t+1}) \max_{a'} Q_{\bar{\theta}}(s_{t+1}, a'), \\
\mathcal{L}(\theta) &= \mathbb{E}_{(s_t,a_t,r_t,s_{t+1},d_{t+1})\sim \mathcal{D}}
\left[ \ell_\kappa\big(y_t - Q_{\theta}(s_t,a_t)\big) \right],
\end{align}
where $\ell_\kappa(\cdot)$ is the Huber loss for robustness to heavy-tailed TD errors. The discount $\gamma\in(0,1)$ should reflect the trading horizon (e.g., $\gamma\in[0.95,0.999]$ for intraday vs. swing horizons).

\paragraph{Double DQN.}
To reduce overestimation, we use Double DQN with decoupled action selection/evaluation:
\begin{equation}
\label{eq:double-dqn}
y_t^{\text{DDQN}} = r_t + \gamma (1-d_{t+1})\;
Q_{\bar{\theta}}\!\left(s_{t+1}, \arg\max_{a'} Q_\theta(s_{t+1},a')\right).
\end{equation}

\paragraph{Architecture.}
For time-series, $Q_\theta$ can be (i) a 1D-CNN over the window $w$, (ii) an LSTM/GRU encoder with the last hidden state feeding an MLP head that outputs $|\mathcal{A}|$ Q-values, or (iii) a Transformer encoder for long-range dependencies. Layer normalization and dropout mitigate non-stationarity; \texttt{ReLU}/\texttt{GELU} activations are typical.

\paragraph{Stabilization.}
We employ (i) target network smoothing with soft updates
$\bar{\theta} \leftarrow \tau \theta + (1-\tau)\bar{\theta}$, $\tau\!\ll\!1$,
(ii) prioritized replay with sampling probability $\propto | \delta_t |^\alpha$ and importance weights, and (iii) action masking when risk or inventory limits are hit.

\subsection{Exploration and Constraints}
We use $\epsilon$-greedy with linear or cosine decay from $\epsilon_{\max}$ to $\epsilon_{\min}$; in practice, \emph{noisy layers} can replace explicit $\epsilon$ for state-dependent exploration. Trading-specific constraints (max position $K$, max turnover, exposure to news embargo windows) are enforced via an action mask $m_t(a)\in\{0,1\}$ and
\[
a_t \;=\; \arg\max_{a \in \mathcal{A}: m_t(a)=1} Q_\theta(s_t,a).
\]

\subsection{Training Algorithm}

\subsection{Practical Considerations for Time-Series}
\paragraph{Data Splitting \& Leakage.}
Use chronological splits and \emph{walk-forward} evaluation: rolling train/validation windows with a held-out test period. All preprocessing (scalers, PCA, feature selection) must be fit on training only and applied forward.

\paragraph{Stationarity \& Regimes.}
Markets are non-stationary; periodic target network updates ($\tau$) and shorter replay horizons help. Consider re-training or fine-tuning across regimes and adding a \emph{regime feature} (e.g., volatility state) to $s_t$.

\paragraph{Costs \& Slippage.}
Model costs explicitly in $r_t$ and optionally inject execution noise during training to bridge the sim-to-real gap. Limit turnover via the $|\Delta a_t|$ penalty.

\paragraph{Risk Controls.}
In addition to reward penalties, enforce hard caps: max position $K$, max leverage, and a circuit breaker when rolling drawdown exceeds $\mathrm{DD}_{\max}$.

\subsection{Evaluation Metrics}
Let $\{R_t\}$ be realized returns from the executed strategy. Report:
\begin{itemize}
  \item Annualized Sharpe: $\displaystyle \mathrm{SR}=\frac{\sqrt{A}\,\mathbb{E}[R_t]}{\mathrm{Std}[R_t]}$ with $A$ the periods-per-year factor.
  \item Sortino, Calmar, hit ratio, average trade, turnover, max drawdown, and profit factor.
  \item Stability: rolling SR and drawdown; sensitivity to cost $c$; ablations (no costs, no risk term, no mask).
\end{itemize}
Always compare to baselines (buy-and-hold, momentum/mean-reversion heuristics) and include a \emph{purged, embargoed} cross-validation if you use overlapping windows.

\subsection{Model Variants (Optional)}
\begin{itemize}
  \item \textbf{Dueling DQN:} Decompose $Q_\theta(s,a)=V_\theta(s)+A_\theta(s,a)-\frac{1}{|\mathcal{A}|}\sum_{a'}A_\theta(s,a')$ to stabilize value estimation.
  \item \textbf{N-step Returns:} Replace $y$ with $n$-step target $r_t+\gamma r_{t+1}+\cdots+\gamma^{n-1} r_{t+n-1}+\gamma^n \max_{a'} Q_{\bar{\theta}}(s_{t+n},a')$.
  \item \textbf{Distributional RL:} Learn the return distribution $Z(s,a)$ for better risk-sensitive control.
  \item \textbf{Noisy Nets:} Parameterized noise in linear layers for exploration without $\epsilon$-schedules.
\end{itemize}

\subsection{Reference Hyperparameters (Typical Ranges)}
\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Typical value \\ \midrule
Window length $w$ & 32--256 steps \\
Discount $\gamma$ & 0.95--0.999 \\
Optimizer / LR & Adam, $10^{-4}$ to $3\cdot 10^{-4}$ \\
Batch size $B$ & 64--256 \\
Replay size $|\mathcal{D}|$ & $10^5$--$10^6$ \\
Target update & soft $\tau \in [10^{-3}, 10^{-2}]$ (or hard every 1--5k steps) \\
$\epsilon$ schedule & from 1.0 to 0.05 over $10^5$ steps (or NoisyNets) \\
Cost $c$ & set by venue; stress $\times 2\text{--}4$ for robustness \\
Risk weights & $\lambda_{\mathrm{risk}}, \lambda_{\mathrm{dd}}$ via grid search on validation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}
DQN assumes a stationary $Q^\star$ and Markovian dynamics, both often violated in markets. Performance can degrade under regime shifts, changing costs/liquidity, or adversarial feedback. Robustness checks (stress costs, volatility spikes, delayed fills) and conservative deployment (small capital, shadow trading) are essential.

\section{Transformer-Based Deep Q-Learning for Time-Series Trading}
\label{sec:transformer-dqn-trading}

\subsection{Problem Formulation}
We frame single-asset trading as a finite-horizon Markov Decision Process (MDP) over price and feature sequences $\{o_t\}_{t=1}^T$. At each time $t$, the agent observes a state $s_t \in \mathcal{S}$, selects an action $a_t \in \mathcal{A}$, receives reward $r_t$, and transitions to $s_{t+1}$. 

\paragraph{State.}  
The state is a sequence of past observations:
\[
s_t = \big[\phi(o_{t-w+1}), \dots, \phi(o_t)\big] \in \mathbb{R}^{w \times d},
\]
where $w$ is the window size, $d$ the feature dimension, and $\phi(\cdot)$ includes log-returns, volatility, and other technical indicators.

\paragraph{Action Space.}  
We use a discrete set of position actions:
\[
\mathcal{A} = \{-K, \dots, -1, 0, 1, \dots, K\},
\]
representing target positions (short to long) subject to max position $K$.

\paragraph{Reward.}  
The reward accounts for P\&L, trading costs, and risk:
\[
r_t = a_t \cdot \Delta P_{t+1} - c |\Delta a_t| - \lambda_{\mathrm{risk}} \hat{\sigma}_t^2 - \lambda_{\mathrm{dd}} \max(0, \mathrm{DD}_t - \mathrm{DD}_{\max}).
\]

\subsection{Transformer Q-Network}
Instead of a traditional CNN/LSTM, we use a Transformer encoder to model long-range dependencies in time-series. The network $Q_\theta(s_t, a_t)$ is parameterized as:

\begin{itemize}
    \item Input: sequence of feature vectors $s_t$.
    \item Positional encodings added to preserve temporal order.
    \item Stacked Transformer encoder layers with multi-head attention.
    \item Final MLP head outputs $|\mathcal{A}|$ Q-values.
\end{itemize}

Formally, let $\text{Transformer}_\theta(\cdot)$ denote the output embedding for the last token:
\[
Q_\theta(s_t, a) = \text{MLP}_\theta\big(\text{Transformer}_\theta(s_t)\big)_a.
\]

\subsection{Training with Double DQN Targets}
The Transformer Q-network is trained using Double DQN targets:
\[
y_t = r_t + \gamma (1 - d_{t+1}) Q_{\bar{\theta}}\Big(s_{t+1}, \arg\max_{a'} Q_\theta(s_{t+1}, a')\Big),
\]
where $\bar{\theta}$ is a target network updated softly: $\bar{\theta} \gets \tau \theta + (1-\tau)\bar{\theta}$.

The loss is the Huber loss over a replay buffer $\mathcal{D}$:
\[
\mathcal{L}(\theta) = \mathbb{E}_{(s_t,a_t,r_t,s_{t+1},d_{t+1}) \sim \mathcal{D}} \big[ \ell_\kappa(y_t - Q_\theta(s_t, a_t)) \big].
\]

\subsection{Exploration and Constraints}
\begin{itemize}
    \item \textbf{Exploration:} $\epsilon$-greedy or parameter noise (NoisyNet layers) in the MLP head.
    \item \textbf{Constraints:} Action masks enforce max position, turnover, and risk limits.
\end{itemize}

\subsection{Practical Considerations}
\begin{itemize}
    \item \textbf{Data Splitting:} Walk-forward evaluation to prevent lookahead bias.
    \item \textbf{Stationarity:} Transformers can capture longer temporal dependencies but may still require retraining on regime shifts.
    \item \textbf{Costs \& Slippage:} Include in the reward function to improve robustness.
    \item \textbf{Hyperparameters:} Window length $w$, number of Transformer layers, number of attention heads, hidden dimensions, learning rate, batch size, replay buffer size.
\end{itemize}

\subsection{Evaluation Metrics}
Use the same trading metrics as before: Sharpe, Sortino, maximum drawdown, hit ratio, turnover, and profit factor. Compare against buy-and-hold and heuristic baselines.

\subsection{Remarks}
Replacing the RNN/CNN with a Transformer enables the agent to capture longer-range dependencies in time-series, which is beneficial for assets with complex temporal patterns or irregular cycles. Care must be taken to limit overfitting due to increased model capacity.



\begin{algorithm}[H]
\caption{Deep Q-Learning Training}
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}




\newpage

\chapter{Week 12}
\minitoc

\newpage
\blankpage

\newpage


\section{Deep Q-Learning}

\subsection{Deep Q-Learning introduction}

\indent In order to approximate optimal decision-making in complex environments, we employ Deep Q-Learning, an extension of Q-Learning that integrates state discretization with function approximation techniques. Unlike traditional tabular Q-Learning, which directly maintains a Q-table over discrete states, Deep Q-Learning is capable of handling continuous or high-dimensional state spaces by discretizing them into manageable bins. The algorithm balances exploration and exploitation using an $\epsilon$-greedy strategy with exponential decay, ensuring sufficient exploration during early episodes while gradually converging toward exploitation of learned policies. At each training step, the Q-table is updated via the Bellman equation, incorporating observed rewards and estimated future returns. The following pseudocode outlines the full training procedure.

\vfill

\begin{algorithm}[H]
\caption{Deep Q-Learning Training}
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}

\vfill

\newpage

\subsection{Deep Q-Learning Process}

\indent To implement this method, we will use the same Q-Learning algorithm object and change during the training to a Neural Network. As a first example we will use a MLP.

\begin{center}
\begin{tikzpicture}[
    node distance=2.5cm,
    box/.style={
        draw,
        rounded corners,
        minimum width=2.5cm,
        minimum height=1cm,
        align=center,
        font=\small,
        fill=blue!10
    },
    arrow/.style={
        -{Stealth[length=2mm]},
        thick
    }
]

% Nodes
\node[box] (state) {State};
\node[box, right=of state] (dqn) {DeepQ*Learning};
\node[box, above right=1.5cm and 2.5cm of dqn] (sell) {Sell};
\node[box, right=of dqn] (hold) {Hold};
\node[box, below right=1.5cm and 2.5cm of dqn] (buy) {Buy};

% Connectors and Labels
\coordinate (connect) at ($(dqn)!0.5!(hold)$);
\draw[arrow] (state) -- (dqn.west);
\draw[arrow] (connect) |- (sell);
\draw[arrow] (connect) |- (buy);
\draw[arrow] (dqn.east) -- (hold);

\end{tikzpicture}
\end{center}

\subsection{MLP layer}

\indent The Multi-Layer Perceptron (MLP) layer serves as the core of the neural network, responsible for learning and transforming input data through a series of connected, dense layers. It is composed of multiple fully-connected layers, each followed by a non-linear activation function. This non-linearity is crucial as it allows the network to model complex, non-linear relationships in the data that a simple linear model could not capture.\par


\indent The \texttt{MLP} class is defined as a PyTorch module, which is a standard approach for building neural network components. The constructor, \texttt{\_\_init\_\_}, initializes the network's architecture. It takes three key arguments: \texttt{input\_dim} (the dimensionality of the input data), \texttt{output\_dim} (the number of output units), and \texttt{hidden\_dims} (a tuple specifying the number of units in each hidden layer, which defaults to \texttt{(128, 128)}). The implementation uses a loop to dynamically build the hidden layers. For each dimension specified in \texttt{hidden\_dims}, it adds a \texttt{nn.Linear} layer (a fully-connected layer) followed by a \texttt{nn.ReLU} activation function. The Rectified Linear Unit (ReLU) is chosen for its computational efficiency and its effectiveness in preventing the vanishing gradient problem.\par

\indent After the hidden layers are constructed, a final \texttt{nn.Linear} layer is added. This last layer maps the output of the final hidden layer to the desired \texttt{output\_dim} of the network. The entire sequence of layers is then encapsulated into a single \texttt{nn.Sequential} container, which ensures that the data will be passed through the layers in the correct order during the forward pass.\par

\begin{table}[h!]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type}\\
        \midrule
        \texttt{nn.Linear} & Linear Layer \\
        \texttt{nn.ReLU} & Activation Function \\
        \texttt{nn.Sequential} & Container \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the MLP layers}
    \label{tab:mlp_layers}
\end{table}

\newpage


\indent After a 2-hour training period, the model's performance was evaluated on a test dataset. The results are visualized in the following plot :

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/MLP_result/QLearning_BTC-USD_2014.png}
    \caption{Performance of the Q-Learning Model on a BTC-USD test set}
    \label{fig:performance_plot}
\end{figure}

\indent The plot demonstrates that the model was able to generate positive returns, indicating a degree of success in the training. However, it is critical to note that this test was conducted on data from an overall uptrend market. This suggests the model's profitability may be a result of the market's general direction rather than its ability to make strategic decisions in both bullish and bearish conditions.\par

\indent A closer inspection of the training log and the trading history reveals that the model made a significant number of "illegal moves." These unauthorized actions, which violate the predefined trading constraints or rules of the environment, are a clear indication of a failure in the model's policy and training process. This result suggests that while the model found a way to profit, it did so by exploiting an oversight in the environment or by not correctly learning the full set of trading rules. Further work is required to correct these behaviors and ensure the model operates within the defined constraints, leading to a more robust and generalizable trading policy.\par


\indent To further analyze the model's performance across different market conditions, we present additional trading simulations on various stock datasets :

\vfill

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/MLP_result/QLearning_AAPL_2010.png}
    \caption{Performance on Apple (AAPL) data from 2010.}
\end{figure}

\vfill

\newpage

\begin{figure}[h]
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_ATOS_2017.png}
        \caption{Performance on ATOS data from 2017.}
        \label{fig:performance_atos}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_O_2016.png}
        \caption{Performance on Realty Income (O) data from 2016.}
        \label{fig:performance_o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_RNO_2016.png}
        \caption{Performance on Renault (RNO) data from 2016.}
        \label{fig:performance_rno}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_TSLA_2019.png}
        \caption{Performance on Tesla (TSLA) data from 2019.}
        \label{fig:performance_tsla}
    \end{subfigure}

    \caption{Model performance across various market conditions.}
    \label{fig:performance_plots_all}
\end{figure}

\indent The plots demonstrate that the model was able to generate positive returns, indicating a degree of success in the training. However, it is critical to note that the test on the BTC-USD data was conducted on an overall uptrend market. This suggests the model's profitability may be a result of the market's general direction rather than its ability to make strategic decisions in both bullish and bearish conditions.\par

\noindent A closer inspection of the training log and the trading history reveals that the model made a significant number of "illegal moves." These unauthorized actions, which violate the predefined trading constraints or rules of the environment, are a clear indication of a failure in the model's policy and training process. This result suggests that while the model found a way to profit, it did so by exploiting an oversight in the environment or by not correctly learning the full set of trading rules. Further work is required to correct these behaviors and ensure the model operates within the defined constraints, leading to a more robust and generalizable trading policy.


\subsection{Decision Transformer Layer}

\indent Unlike a traditional Multi-Layer Perceptron (MLP) that processes a single state at a time, the DecisionTransformerQ layer is designed to handle sequential data, leveraging the powerful architecture of a transformer. This approach frames reinforcement learning as a sequence modeling problem, where the model learns to predict future actions based on a history of past states and desired returns.\par

\noindent The \texttt{DecisionTransformerQ} class is built upon a pre-trained transformer model from Hugging Face's library, which serves as the core `backbone`. The constructor, \texttt{\_\_init\_\_}, initializes a configuration for the transformer and loads the corresponding model (`DecisionTransformerGPT2Model`). This approach allows the network to benefit from the pre-trained weights, which are already effective at capturing complex patterns in sequential data.\par

\noindent Before the input is fed into the transformer, it passes through a `self.input\_proj` linear layer. This layer's purpose is to project the raw input data, which has a dimensionality of \texttt{input\_dim}, into an embedding space that matches the transformer's hidden size. This ensures the input is correctly formatted for the transformer architecture.\par

\noindent The \texttt{forward} method outlines the data flow. First, it ensures the input tensor \texttt{x} is in the correct shape for the transformer (a 3D tensor representing a batch, sequence of steps, and features). The input is then passed to the `self.input\_proj` layer for projection. The core of the computation happens when the projected data is fed into the transformer `backbone`. The transformer processes the entire sequence and its output's `last\_hidden\_state` is used. This final hidden state is a rich representation that has attended to the entire sequence history, making it ideal for the final decision. Finally, the `self.q\_head` linear layer takes this comprehensive representation and projects it to the desired \texttt{output\_dim}, providing the predicted Q-values.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type} \\
        \midrule
        \texttt{DecisionTransformerGPT2Model} & Transformer Backbone \\
        \texttt{nn.Linear} (Input) & Linear Layer \\
        \texttt{nn.Linear} (Q-Head) & Linear Layer \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the Decision Transformer layers}
\end{table}


\indent After a 4-hour training period, the model's performance was evaluated on a test dataset. The results are visualized in the following plot:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/GPT_transformer_results/DeepQLearning_O_2016_v2.png}
    \caption{Performance of the Deep Q-Learning Model on Realty Income (O) data from 2016.}
    \label{fig:deep_q_performance}
\end{figure}

\indent This model achieved its best results on the Realty Income (O) stock during both training and testing. While the model was able to generate positive returns, further analysis is required because the stock's volatility is high due to a large number of trades. The next step will be to add broker fees to the model to limit the number of trades and make the results more representative of real-world trading conditions.\par

\newpage

\chapter{Week 13}
\minitoc

\newpage
\blankpage

\newpage


\section{Deep Q-Learning}
\subsection{Decision Transformer Layer}

\indent Unlike a traditional Multi-Layer Perceptron (MLP) that processes a single state at a time, the DecisionTransformerQ layer is designed to handle sequential data, leveraging the powerful architecture of a transformer. This approach frames reinforcement learning as a sequence modeling problem, where the model learns to predict future actions based on a history of past states and desired returns.\par

\noindent The \texttt{DecisionTransformerQ} class is built upon a pre-trained transformer model from Hugging Face's library, which serves as the core `backbone`. The constructor, \texttt{\_\_init\_\_}, initializes a configuration for the transformer and loads the corresponding model (`DecisionTransformerGPT2Model`). This approach allows the network to benefit from the pre-trained weights, which are already effective at capturing complex patterns in sequential data.\par

\noindent Before the input is fed into the transformer, it passes through a `self.input\_proj` linear layer. This layer's purpose is to project the raw input data, which has a dimensionality of \texttt{input\_dim}, into an embedding space that matches the transformer's hidden size. This ensures the input is correctly formatted for the transformer architecture.\par

\noindent The \texttt{forward} method outlines the data flow. First, it ensures the input tensor \texttt{x} is in the correct shape for the transformer (a 3D tensor representing a batch, sequence of steps, and features). The input is then passed to the `self.input\_proj` layer for projection. The core of the computation happens when the projected data is fed into the transformer `backbone`. The transformer processes the entire sequence and its output's `last\_hidden\_state` is used. This final hidden state is a rich representation that has attended to the entire sequence history, making it ideal for the final decision. Finally, the `self.q\_head` linear layer takes this comprehensive representation and projects it to the desired \texttt{output\_dim}, providing the predicted Q-values.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type} \\
        \midrule
        \texttt{DecisionTransformerGPT2Model} & Transformer Backbone \\
        \texttt{nn.Linear} (Input) & Linear Layer \\
        \texttt{nn.Linear} (Q-Head) & Linear Layer \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the Decision Transformer layers}
    \label{tab:decision_transformer_layers}
\end{table}

\subsection{GPT2 Transformer description}

\vspace{0.5cm}

\begin{center}
\begin{tikzpicture}[
    scale=0.70,
    transform shape,
    node distance=0.5cm,
    box/.style={
        draw,
        rounded corners,
        minimum width=2.5cm,
        minimum height=1cm,
        align=center,
        font=\small,
        fill=blue!10
    },
    arrow/.style={
        -{Stealth[length=2mm]},
        thick
    }
]

% Nodes
\node[box, fill=blue!30] (input) {INPUT};
\node[box, right=of input, scale=0.8] (embedding) {Embedding};
\node[box, right=0.2cm of embedding, scale=0.8] (drop) {Drop Out};
\node[box, right=of drop, fill=gray, scale=1.4] (attention) {GPT Attention};
\node[box, right=of attention, scale = 0.8] (normalization1) {Normalization};
\node[box, right=of normalization1, fill=gray, scale=1.4] (transformer) {GPT Transformer};
\node[box, right=of transformer, scale=0.8] (normalization2) {Normalization};
\node[box, right=of normalization2, fill=blue!30] (out) {OUTPUT};

% Connectors and Labels
\draw[arrow] (input) -- (embedding);
\draw[arrow] (embedding) -- (drop);
\draw[arrow] (drop) -- (attention);
\draw[arrow] (attention) -- (normalization1);
\draw[arrow] (normalization1) -- (transformer);
\draw[arrow] (transformer) -- (normalization2);
\draw[arrow] (normalization2) -- (out);

\end{tikzpicture}
\end{center}

\noindent As we can see, the GPT2 transformer is a basic transformer, however the good performance of this particular transformer is that the model has been created using the GPT2 opensource transformer available on hugging face. Here are some results of the model on different stocks.

\newpage

\vfill
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_AAPL_2010.png}
    \caption{Performance of the Transofrmer Model on Apple data from 2010.}
\end{figure}
\vfill
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_O_2016_v2.png}
    \caption{Performance of the Transofrmer Model on Realty income data from 2016.}
\end{figure}
\vfill
\newpage
\vfill
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_renault_2016.png}
    \caption{Performance of the Transofrmer Model on Renault data from 2016.}
\end{figure}
\vfill
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_TSLA_2019.png}
    \caption{Performance of the Transofrmer Model on Tesla data from 2019.}
\end{figure}
\vfill

\indent As we can see, there is a too many trades that are made during the training and testing part, this is an issue caused by the penalty of the model ; there is no penalty on the number of trade the bot will make, this suggest that we need to find a way to make the bot understand that a trade should be made wiser. For that we suggest to add the broker's commission for a buy or sell action. Let's take a 10\% commission which make the bot considering the number of trade made.

\vfill

\newpage

\subsection{Result for QLearning (with commission)}

\indent We changed the environment behavior to provide a 10 \% commission for each buy and sell movements. Also we changed the reward to provide only the difference between the initial wallet et the current portfolio so that the agent learn also on the market behavior directly without any outside penalties that could makes him learn false dynamic. The problem here could be that the agent does not learn about the rules of the set of action. For example, the issue here could be that the model do lot of illegals moves such as buying twice in a row, if that happen, we will change the reward back with penalties. Here are the results on the QLearning agent :

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_AAPL_2010_commission.png}
    \caption{Performance of the QLearning Model on Apple data from 2010 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_BTC-USD_2014_commission.png}
    \caption{Performance of the QLearning Model on Bitcoin data from 2014 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_TSLA_2019_commission.png}
    \caption{Performance of the QLearning Model on Tesla data from 2019 with commission}
\end{figure}

\noindent As we can see the number of trades reduced, however the performance is still poor since it is still following the trend of the actual stock. This last observation could be explained by the fact that QLearning is a basic Machine Learning algorithm.


\subsection{Result for DeepQLearning - GPT Transformer (with commission)}

\indent We did the same experiment on the transformer model. However this model takes a while to train, we could not have a lot of output results so we will analyze only two experiments :

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{../img/expriment_on_commission/DeepQLearning_O_2016_commission.png}
    \caption{Performance of the Transformer Model on Real income data from 2016 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{../img/expriment_on_commission/DeepQLearning_TSLA_2019_commission.png}
    \caption{Performance of the Transformer Model on Tesla data from 2019 with commission}
\end{figure}

\noindent As we can see the performance on the training part of Real income is really good, the agent seems to understand the dynamic of the stock as a comparison, the current market made 80\% profit on this period while our model made 98\%, which means that on the training part our model get very well the dynamic aspect of the market. However we can see the limitation of the model during the testing part where our model made -2\% and the current market lost 6\% which indicates that our model also outperform the market but made us lose money still.\par

\noindent On another hand, on the Tesla market, the agent has completely been broke on his behaviour. The difference with the previous agent is that the environement is giving a reward based on the difference in between the initial wallet and the actual portfolio, which later in a report, is suggested as a better reward based for a trading bot using transformer. This could also be an issue from the transofrmer model itself (bad intergation of the model / bad purpose of the model), we will analyze this issue later.

\newpage

\section{Paper : Transformers in Reinforcement Learning : A survey}


\indent Transformers have emerged as a dominant architecture in modern machine learning, revolutionizing natural language processing, computer vision, and a growing number of other domains. Their ability to model long-range dependencies, capture contextual information, and scale effectively has prompted increasing interest in applying them to reinforcement learning (RL). In RL, agents must learn sequential decision-making policies under uncertainty, a setting that aligns naturally with the sequence modeling capabilities of transformers. Over the past few years, researchers have proposed a variety of transformer-based methods for RL tasks, ranging from policy optimization and value estimation to model-based approaches.\par

\noindent This survey, written in 2023 by Pranav Agarwal (cole de Technologie Suprieure/Mila, Canada), Aamer Abdul Rahman (cole de Technologie Suprieure/Mila, Canada), Pierre-Luc St-Charles (Mila, Applied ML Research Team, Canada), Simon J.D. Prince (University of Bath, United Kingdom) and Samira Ebrahimi Kahou (cole de Technologie Suprieure/Mila/CIFAR, Canada), provides a comprehensive overview of this rapidly developing research area. The paper reviews how transformers have been adapted for reinforcement learning, highlights emerging architectures and methodologies, and categorizes them according to their applications. It also discusses empirical results, key challenges such as scalability and sample efficiency, and potential future research directions.\par

\noindent By situating transformer-based RL within the broader landscape of sequence modeling and decision-making, this survey contributes to a deeper understanding of both the opportunities and limitations of applying transformers in reinforcement learning. It serves as a reference point for researchers and practitioners seeking to build on the foundations of this promising intersection of fields. This paper talk about the transformer on many different fields but the interesting part for us is the trading part.

\subsection{Transformer RL in Trading}

\indent Portfolio optimization aims to balance returns and risks when selecting assets but this is difficult due to market volatility and external factors. Reinforcement learning (RL) has been explored to automate trading decisions by learning from historical market data, such as price trends, volumes and sentiment.\par

\noindent Transformers are particularly suited for this task because they can capture both sequential patterns in asset prices and correlations between different assets. It has been introduced that the Relation-Aware Transformer (RAT) for portfolio selection, where the encoder extracts sequential and relational features, and the decoder makes trading decisions (including leverage and short sales). The approach was evaluated on real-world stock and cryptocurrency data, showing competitive performance compared to state-of-the-art portfolio selection methods. The given model would is described as an encoder-decoder transformer like so :

\vspace{1cm}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        scale=0.9,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={
                -{Stealth[length=2mm]},
        thick
        }
        ]
        
        % Nodes
        \node[box, fill=blue!30] (input) {INPUT};
        \node[box, right=of input, scale=0.8] (embedding) {Embedding};
        \node[box, right=of embedding, fill=gray, scale=1.4] (encoder) {Encoder};
        \node[box, right=of encoder, fill=gray, scale=1.4] (decoder) {Decoder};
        \node[box, right=of decoder, scale=0.8] (normalization) {Normalization};
        \node[box, right=of normalization, fill=blue!30] (out) {OUTPUT};
        
        % Connectors and Labels
        \draw[arrow] (input) -- (embedding);
        \draw[arrow] (embedding) -- (encoder);
        \draw[arrow] (encoder) -- (decoder);
        \draw[arrow] (decoder) -- (normalization);
        \draw[arrow] (normalization) -- (out);
        
    \end{tikzpicture}
    \caption{Encoder-Decoder Stack of the transofrmer model}
\end{figure}

\newpage

\noindent Let us now dive into the encoder stack to understand how these sequential and relational features are extracted and represented for reinforcement learning-based portfolio optimization.

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[
        scale=1,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={-{Stealth[length=2mm]}, thick},
            line/.style={thick}
            ]

        % Nodes
        \node[box, fill=blue!30] (input) {INPUT};
        \node[coordinate, above=of input] (junction1) {} ;
        \node[box, above=of junction1,fill=gray ,scale=1.4] (mha) {Multi-Head\\ Attention};
        \node[box, above=of mha, scale=0.9] (AddNorm1) {Add \& Norm};
        \node[coordinate, above=of AddNorm1] (junction2) {} ;
        \node[box, above=of junction2] (ff) {Feed Forward} ;
        \node[box, above=of ff] (AddNorm2) {Add \& Norm} ;
        \node[box, above=of AddNorm2, fill=blue!30] (output) {OUTPUT};

        % Connectors
        \draw[line] (input.north) -- (junction1);
        \draw[arrow] (junction1) -- (mha);
        \draw[arrow] (mha) -- (AddNorm1);
        \draw[arrow] (junction1.west) -- ++(-2.5, 0) |- (AddNorm1.west);
        \draw[arrow] (AddNorm1) -- (junction2);
        \draw[line] (junction2) -- (ff);
        \draw[arrow] (ff) -- (AddNorm2);
        \draw[arrow] (junction2.west) -- ++(-2.5, 0) |- (AddNorm2.west);
        \draw[arrow] (AddNorm2) -- (output);

\end{tikzpicture}
\caption{Encoder stack}
\vspace{-.8cm}
\end{wrapfigure}

\indent In the context of trading and portfolio optimization, the encoder plays a crucial role in capturing the complex patterns present in historical market data. It processes sequences of asset prices, trading volumes and other market indicators to model both short-term trends and long-term dependencies. By leveraging the self-attention mechanism inherent to transformers, the encoder can identify important relationships not only within a single asset's time series but also across multiple assets, effectively capturing correlations that are critical for informed trading decisions.\par

\noindent In the context of a transformer model, we are using the Attention architecture to capture temporal dependencies and correlations in sequential trading data. The attention mechanism allows the model to weigh the importance of past states and actions when predicting future trading decisions, which is particularly useful in financial markets where certain events or trends can have long-term effects. The Multi-Head Attention (MHA) layer enables the model to simultaneously focus on different aspects of the input sequences, such as price trends, trading volumes and market sentiment. By splitting the attention into multiple heads, the network can learn diverse representations of the data, improving its ability to capture complex relationships between assets.\par

\noindent Let's now describe the two main component of these layers which are the Multi-Head Attention layer and the Feed Forward layer. We will also describe the basis of original Attention layer that is included into the Multi-Head Attention :

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        scale=0.7,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={-{Stealth[length=2mm]}, thick},
            line/.style={thick}
            ]

        % Nodes
        \node[box, fill=blue!30, minimum width=12cm] (input) {INPUT};

        %Attention head #1
        \node[box, above=2cm of input.west] (linear1) {Linear};
        \node[box, above=of linear1, fill=orange!80, scale=1.4] (attention1) {\\Attention \#1\\ \\ $softmax\left( \frac{QK^T}{\sqrt{d_k}} \right)V$};

        %Dot x8
        \node[above=4cm of input, scale=1.4] (dots) {$\cdots\vspace{3mm}\times 8$};

        %Attention head #8
        \node[box, above=2cm of input.east] (linear8) {Linear};
        \node[box, above=of linear8, fill=orange!80, scale=1.4] (attention8) {\\Attention \#8\\ \\ $softmax\left( \frac{QK^T}{\sqrt{d_k}} \right)V$};

        % Output
        \node[box, above=7cm of input, minimum width=12cm] (concat) {Concat};
        \node[box, above=of concat] (linear3) {Linear};
        \node[box, above=of linear3, fill=blue!30] (output) {OUTPUT};

        % ========================================================================================

        % Connectors
        \draw[arrow]  ([xshift=-20mm]input.north)  -- ++(0, 0.5) -|  (linear1) ;
        \draw[dashed] ([yshift=+5mm]input.north) -- ++(-0.5,0.5)  --   ([xshift=-5.3mm ,yshift=-8mm]dots.south) ;
        \draw[dashed] (input.north)  --   ([yshift=-10mm]dots.south) ;
        \draw[dashed] ([yshift=+5mm]input.north) -- ++(0.5,0.5)  --   ([xshift=+5.3mm ,yshift=-8mm]dots.south) ;
        \draw[arrow]  ([xshift=20mm]input.north)  -- ++(0, 0.5) -|  (linear8) ;
    
        %Attention head #1
        \draw[arrow] ([xshift=5mm]linear1.north west)  --  ([xshift=15mm]attention1.south west) node[midway, right] {$Q_1$};
        \draw[arrow] (linear1.north)                   --  (attention1) node[midway, right] {$H_1$};
        \draw[arrow] ([xshift=-5mm]linear1.north east) --  ([xshift=-15mm]attention1.south east) node[midway, right] {$V_1$};
        \draw[arrow] (attention1.north)                -- ++(0, 0.5) -|  ([xshift=20mm]concat.south west) ;
    
        %Dot x8
        \draw[dashed] ([xshift=-5.3mm, yshift=+8mm]dots.north) -- ([xshift=-5.3mm]concat.south) ;
        \draw[dashed] ([yshift=+10mm]dots.north)  --   (concat) ;
        \draw[dashed] ([xshift=+5.3mm ,yshift=+8mm]dots.north) -- ([xshift=+5.3mm]concat.south) ;
    
        %Attention head #2
        \draw[arrow] ([xshift=5mm]linear8.north west)  --  ([xshift=15mm]attention8.south west) node[midway, right] {$Q_8$};
        \draw[arrow] (linear8.north)                   --  (attention8) node[midway, right] {$H_8$};
        \draw[arrow] ([xshift=-5mm]linear8.north east) --  ([xshift=-15mm]attention8.south east) node[midway, right] {$V_8$};
        \draw[arrow] (attention8.north)                -- ++(0, 0.5) -|  ([xshift=-20mm]concat.south east) ;


        \draw[arrow] (concat)                          --  (linear3) ;
        \draw[arrow] (linear3)                         --  (output) ;

\end{tikzpicture}
\caption{Multi-Head Attention stack (x8 heads)}
\vspace{-.8cm}
\end{figure}

\newpage

\vspace{2cm}

\begin{figure}[h]
    \centering
    \begin{minipage}{.45\linewidth}
        \centering
        \begin{tikzpicture}[
            scale=1,
            transform shape,
            node distance=0.5cm,
            box/.style={
                draw,
                rounded corners,
                minimum width=3cm,
                minimum height=1cm,
                align=center,
                font=\small,
                fill=blue!10
                },
                arrow/.style={-{Stealth[length=2mm]}, thick},
                line/.style={thick}
                ]

            % Nodes
            \node[box, fill=blue!30] (input) {INPUT};
            \node[box, above=of input.north, minimum width=5cm] (linear) {Linear};
            \node[box, above=of linear.north, anchor=south west, minimum width=1cm] (q) at (linear.north west) {Q};
            \node[box, above=of linear.north, minimum width = 1cm]      (k) {K};
            \node[box, above=of linear.north, anchor=south east, minimum width=1cm] (v) at (linear.north east) {V};
            \node[box, above=of q] (matmul) at ($(q)!0.5!(k) + (0,0.5)$) {matmul};
            \node[box, above=of matmul] (scale) {scale};
            \node[box, above=of scale] (softmax) {softmax};
            \node[box, above=8cm of input, minimum width=5cm] (matmulOut) {matmul};
            \node[box, above=of matmulOut, fill=blue!30] (output) {OUTPUT};

            % Connectors
            \draw[arrow] (input.north) -- (linear);
            \draw[arrow] ([xshift=5mm]linear.north west)  -- (q);
            \draw[arrow] (linear.north) -- (k);
            \draw[arrow] ([xshift=-5mm]linear.north east) -- (v);
            \draw[arrow] (q.north)     -- ([xshift=5mm]matmul.south west);
            \draw[arrow] (k.north)     -- ([xshift=-5mm]matmul.south east);
            \draw[arrow] (matmul)      -- (scale);
            \draw[arrow] (scale)       -- (softmax);
            \draw[arrow] (softmax)     -- ([xshift=-10mm]matmulOut.south);
            \draw[arrow] (v)           -- ([xshift=-5mm]matmulOut.south east);
            \draw[arrow] (matmulOut)   -- (output);

        \end{tikzpicture}
        \caption{Attention stack}
    \end{minipage}\hspace{.5cm}
    \begin{minipage}{.45\linewidth}
        \centering
        \begin{tikzpicture}[
            scale=1.4,
            transform shape,
            node distance=0.5cm,
            box/.style={
                draw,
                rounded corners,
                minimum width=2.5cm,
                minimum height=1cm,
                align=center,
                font=\small,
                fill=blue!10
                },
                arrow/.style={-{Stealth[length=2mm]}, thick},
                line/.style={thick}
                ]

                % Nodes
                \node[box, fill=blue!30] (input) {INPUT};
                \node[box, above=of input] (mlp1) {MLP};
                \node[box, above=of mlp1] (relu) {ReLU};
            \node[box, above=of relu] (mlp2) {MLP};
            \node[box, above=of mlp2, fill=blue!30] (output) {OUTPUT};

            % Connectors
            \draw[arrow] (input.north) -- (mlp1);
            \draw[arrow] (mlp1)        -- (relu);
            \draw[arrow] (relu)        -- (mlp2);
            \draw[arrow] (mlp2)        -- (output);

        \end{tikzpicture}
        \caption{Feed-Forward stack}
    \end{minipage}
\end{figure}

\vspace{1cm}

\noindent Finally, after implementing these layers in PyTorch, we will be able to start building the algorithm for a Deep Q-Learning agent using our own model, which should be capable of training on trading markets. The training strategy is as follows: the price assets are used as input to the encoder, which extracts features that are then passed to the decoder and the decision-making layer to produce the final action for a given input. We then compare the resulting portfolio with the initial one and assign a reward based on the evolution of the portfolio's value.

%\newpage

%\appendix


%\newpage

%Rfrences
%\printbibliography

\newpage

%Glossaire
\printacronyms

% \newpage

% \printglossary

% %Annexes
% \section{Annexes}
% \input{annexe.tex}

\end{document}
