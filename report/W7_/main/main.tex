\documentclass[11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[myheadings]{fullpage}
\DeclareUnicodeCharacter{0301}{\hspace{-1ex}\'{ }}

\usepackage{silence}

% Package for headers 
\usepackage{fancyhdr}
\usepackage{lastpage}

% For figures and stuff
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}

% Change for different font sizes and families
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}

% Maths
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Code
\usepackage{minted}
\usepackage{adjustbox}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwProg{While}{while}{}{}
\SetKwProg{For}{for}{}{}
\SetKwProg{Function}{function}{}{}

% Bibliography
\usepackage{biblatex} 
\addbibresource{../references.bib}

%Glossary
\usepackage[acronym]{glossaries}
\makeglossaries
\input{../glossary}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{indentfirst}

%Tabular
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{enumitem}
\usepackage{arydshln}

%Function draw

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{tikz}
\usepackage{tikz-uml}
\usepackage{amsmath}
\usepackage{pgfmath}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\usetikzlibrary{fit, backgrounds, calc}

\usepackage{afterpage}

% Show mini ToCs automatically at the beginning of each chapter
\usepackage{minitoc}
\setcounter{tocdepth}{0}        % Only show chapters in the main ToC
\setcounter{minitocdepth}{2}    % Show sections and subsections in mini ToCs
\dominitoc                      % Enable mini ToCs
\WarningFilter{minitoc(hints)}{W0024}

\definecolor{terminalbg}{RGB}{0,0,0}
\definecolor{terminalfg}{RGB}{255, 255, 255} 

\newminted[console]{bash}{
  bgcolor=terminalbg,
  fontfamily=tt,
  fontsize=\small,
  linenos=false,
  framesep=100mm,
  rulecolor=\color{white},
  baselinestretch=1,
  breaklines=true
}


\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=1.5cm,left=2cm,right=2cm,marginparwidth=1.5cm]{geometry}
% Command to draw a horizontal line
\newcommand{\separator}{%
    \begin{center}
      {\Large$\ast$~$\ast$~$\ast$}
    \end{center}
}


\setlength{\parskip}{10pt}
\setlength{\parindent}{1cm}

\pagestyle{fancy}
\fancyhf{}

% Header and footer information
\setlength\headheight{15pt}
\author{Mathys VINATIER}
%\fancyhead[L]{IPSA} 
\fancyhead[R]{Mathys VINATIER}
\fancyfoot[R]{\thepage}
 \setlength {\marginparwidth }{2cm}
\begin{document}


\date{}

\title{ 
		\HRule{2pt} \\
		\LARGE \textbf{ Report on Kalman Filter applied on Neural Network for Finance } 
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}\\
		Mathys VINATIER  - \href{https://github.com/MathysVinatier/17_SNU_Lab/blob/main/}{GitHub Project page}\\
		Supervisor:      \\
		  Pr Kim Tae-Wan
      }

\maketitle

\newpage
\blankpage
\newpage

\tableofcontents
\blankpage

\newpage
\blankpage
\newpage

\setcounter{chapter}{9}
\chapter{Week 10}
\minitoc

\newpage
\blankpage

\newpage

\section{Reminder on Greedy Policy and QLearning}


Q-Learning is a \textbf{model-free, off-policy, value-based reinforcement learning algorithm} that trains an action-value function (Q-function) to find the optimal policy indirectly. The Q-function estimates the expected cumulative reward of taking a certain action in a given state and following the best policy thereafter.

\begin{itemize}
    \item \textbf{Q-table:} Internally, Q-Learning uses a Q-table storing values for each state-action pair. Initially, all values are zero, and the table is updated iteratively during training.
    \begin{center}
        \begin{tikzpicture}[
            scale=0.7,
            node distance=1.1cm and 2.2cm,
            box/.style={draw, rounded corners, minimum width=2.1cm, minimum height=0.7cm, align=center, font=\small, fill=blue!10},
            arrow/.style={-{Latex[length=2mm]}, thick}
        ]
        % Nodes
        \node[box] (state) {State};
        \node[box, right=of state, yshift=-0.8cm] (qtable) {Q-Table};
        \node[box, below=of state] (action) {Action};
        \node[box, right=of qtable] (qvalue) {Q-Value};
        % Arrows
        \draw[arrow] (state) -- (qtable);
        \draw[arrow] (action) -- (qtable);
        \draw[arrow] (qtable) -- (qvalue);
        % Labels
        \node[left=1mm of qtable] {\scriptsize{Input}};
        \node[right=1mm of qtable, yshift=3mm] {\scriptsize{Output}};   
        \end{tikzpicture}
    \end{center}
    \item \textbf{Epsilon-Greedy Strategy:} To balance exploration and exploitation, actions are chosen using an epsilon-greedy policy:
    \[
        \pi^*(s) = \arg\max_{a} Q^*(s, a)
    \]
        \begin{itemize}
            \item With probability \(1 - \epsilon\), exploit by selecting the action with the highest Q-value.
            \item With probability \(\epsilon\), explore by selecting a random action.
            \item \(\epsilon\) decays over time to shift from exploration to exploitation.
        \end{itemize}
    \begin{center}
        \begin{tikzpicture}[
        scale=0.7,
        node distance=0.5cm and 3cm,
        box/.style={draw, rounded corners, minimum width=3cm, minimum height=0.7cm, align=center, fill=blue!10, font=\small},
        arrow/.style={-{Latex[length=2mm]}, thick}
        ]
        % Nodes
        \node[box] (policy) {\textbf{$\epsilon$-greedy policy}};
        \node[box, above right=of policy] (exploit) {Exploitation\\(greedy action)};
        \node[box, below right=of policy] (explore) {Exploration\\(random action)};
        % Arrows
        \draw[arrow] (policy.east) -- node[above, pos=0.5, rotate=30] {$1-\epsilon$} (exploit.west);
        \draw[arrow] (policy.east) -- node[below, pos=0.5] {$\epsilon$} (explore.west);
        \end{tikzpicture}
    \end{center}
    \item \textbf{TD Update:} At each step, the Q-value for the current state-action pair \(Q(s_t, a_t)\) is updated based on the immediate reward \(r_{t+1}\) plus the discounted maximum Q-value of the next state \(s_{t+1}\):
        \[
        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)\right]
        \]
    \item \textbf{Off-policy Learning:} The policy used to select actions (epsilon-greedy) differs from the policy used to compute the target update (greedy). This distinction allows Q-Learning to learn optimal policies even when exploring randomly.
\end{itemize}

After enough training episodes, the Q-table converges to the optimal Q-function, which directly yields the optimal policy by choosing actions with the highest Q-values in each state.


\newpage

\section{Implementation Progress and Experiments}

\indent This week was dedicated to the initial implementation and experimentation phase of the project, focusing primarily on the Proximal Policy Optimization (PPO) basis algorithms. Using the Hugging Face classes, we developed a prototype with greedy policy agent and integrated it into a custom trading environment inspired by the concepts discussed in previous weeks.

\vspace{0.3cm}

\subsection{Environment Setup}

\indent The custom environment was designed to simulate stock market trading dynamics with the following key elements :

\begin{itemize}
\item \textbf{State representation} including historical price data, technical indicators (moving averages, RSI) and Kalman filtered signals.
\item \textbf{Action space} consisting of discrete trading actions: Buy, Hold or Sell.
\item \textbf{Reward structure} based on portfolio value change and risk-adjusted metrics to incentivize both profit and stability.
\item \textbf{Episode design} with fixed-length trading periods to allow episodic evaluation of agent performance.
\end{itemize}

\noindent The environment was implemented adhering to OpenAI Gym interfaces, allowing smooth integration with standard RL training pipelines. Here is the UML of the created environement :

\vfill

\begin{center}
\begin{tikzpicture}
% TradingEnv class
\umlclass[width=.4\linewidth, x=0, y=0]{TradingEnv}{
    - df : DataFrame \\
    - n\_steps : int \\
    - current\_step : int \\
    - initial\_balance : float \\
    - balance : float \\
    - position : int \\
    - last\_action : int \\
    - observation\_space : Box \\
    - action\_space : Discrete
}{
    + \_\_init\_\_(df) \\
    + \_get\_obs() \\
    + sample\_valid\_action() \\
    + get\_valid\_actions() \\
    + step(action) \\
    + set\_data(df) \\
    + reset()
}

  % QLearning class
  \umlclass[x=8, y=0]{QLearning}{
    - env : TradingEnv \\
    - log : bool \\
    - state\_space : int \\
    - action\_space : int
  }{
    + \_\_init\_\_(env, log=True) \\
    + train(...) \\
    + greedy\_policy(Qtable, state) \\
    + epsilon\_greedy\_policy(Qtable, state\_idx, epsilon) \\
    + discretize\_state(state, bins) \\
    + state\_to\_index(discrete\_state, bins) \\
    + initialize\_q\_table(bins) \\
    + split\_data(df, train\_size) \\
    + get\_actions\_and\_prices(Qtable, df, initial\_cash=100) \\
    + plot(df, Qtable)
  }

  % Association link
  \umlassoc[geometry=-|, arm1=1, arm2=1, anchor1=west, anchor2=east]{QLearning}{TradingEnv}
\end{tikzpicture}
\end{center}

\vfill

\newpage

\begin{algorithm}[H]
\caption{Trading Environment Behavior}
\KwIn{Environment $env$, number of episodes $N$, exploration rate $\epsilon$}
\KwOut{Episode value and rewards}

\For{$episode \leftarrow 1$ \KwTo $N$}{
    $s \leftarrow env.reset()$\;
    total\_reward $\leftarrow 0$\;
    
    \While{True}{
        Get valid actions $A_{valid} \leftarrow env.get\_valid\_actions()$\;
        
            Select $a \leftarrow$ random choice from $A_{valid}$\;
        Execute $(s', r, done, info) \leftarrow env.step(a)$\;
        total\_reward $\leftarrow$ total\_reward $+ r$\;
        
        Log $(episode, s, a, r, info["portfolio\_value"])$\;
        
        $s \leftarrow s'$\;
        
        \If{$done$}{break}
    }
    
    Print episode summary : total\_reward, final portfolio value\;
}
\end{algorithm}

\begin{algorithm}[H]
\caption{Q-Learning Training }
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}

\newpage

\indent In the context of Q-learning for trading environments, bins are used to discretize the continuous state space into a finite number of discrete states. This discretization is essential because the Q-table requires a finite and manageable number of states to store and update the expected rewards for each state-action pair. Continuous variables, such as stock prices or technical indicators, have infinite possible values, which makes it impractical to represent them directly in a Q-table. To address this, we divide the range of each continuous feature into a fixed number of intervals called bins. Each bin represents a segment of the feature's value range, allowing us to map any continuous observation into a discrete category. For implementation, we analyze the training dataset and create evenly spaced bins (10 bins) for each feature by computing linearly spaced intervals between the minimum and maximum values of that feature. Then, during training or evaluation, the observed continuous state is converted into a discrete state by determining which bin each feature value falls into. This discretized state is subsequently converted into a unique index to access and update the Q-table. The use of bins thus enables effective Q-learning in continuous state environments by simplifying the state representation while preserving essential information.

\vspace{0.3cm}

\subsection{Preliminary Results for Q-Learning Greedy Policy}

\indent Initial experiments aimed to evaluate the training behavior and reward progression of the Q-Learning agent following a greedy policy. Results were compared over two time horizons : a shorter training period of 100 episodes and an extended training of 1000 episodes.

\subsubsection{100 episodes}

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2010.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2011.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2012.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2013.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2014.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2015.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2016.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2017.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2018.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2019.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2020.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2021.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_100ep/QLearning_AAPL_2022.png}
\vfill

\subsubsection{1000 episodes}

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2010.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2011.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2012.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2013.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2014.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2015.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2016.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2017.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2018.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2019.png}
\vfill

\newpage

\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2020.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2021.png}
\vfill
\includegraphics[width=.9\linewidth]{../img/QL_1000ep/QLearning_AAPL_2022.png}
\vfill


\begin{itemize}
\item Over 100 episodes, the cumulative rewards exhibited a modest upward trend, indicating initial learning progress, though with significant variability.
\item Extending to 1000 episodes resulted showed that the agent is not taking position anymore, a convergence of the hold position shouyld be evaluated
\item Value estimates became more consistent over time, though some fluctuations persisted due to the noisy environment dynamics (should add filtering or smoothing).
\item The results highlight the trade-off between exploration and exploitation inherent in the greedy approach and motivate future exploration of strategies with controlled exploration. Noticed that the agent is not learning from the path but more on the direct previous value.
\end{itemize}

\noindent These preliminary findings establish a baseline for Q-Learning performance and set the stage for comparisons with other reinforcement learning methods such as Deep Q-Networks (DQN).

\vspace{0.3cm}

\section{Challenges and Solutions}

\indent The Q-Learning experiments faced several challenges specific to the greedy policy and the trading environment :

\begin{itemize}
\item \textbf{Limited exploration due to greediness} : The strictly greedy policy limited the agent's ability to discover better actions, particularly early in training. Future work will include incorporating greedy or softmax action selection to balance exploration.
\item \textbf{Reward noise and sparsity} : The stochastic environment produced noisy and sparse rewards, complicating learning.
\item \textbf{Hyperparameter sensitivity} : Learning rate and discount factor tuning were critical, especially over longer training horizons. We will use optuna to get better hyperparameter in the future on more complexe models.
\item \textbf{Computational efficiency} : Although less demanding than deep RL methods, Q-Learning with large state spaces required careful management of updates. We leveraged experience replay buffers and batch updates to accelerate convergence.
\end{itemize}

\section{Next Steps}

\indent Building on the current progress with Q-Learning and the greedy policy, the immediate next steps focus on advancing the model complexity and preparing for PPO :

\begin{itemize}
\item Designing and integrating a deep neural network architecture to implement Deep Q-Learning (DQN), enabling function approximation for larger and continuous state spaces.
\item Conducting experiments to compare the performance of DQN against the baseline Q-Learning greedy policy.
\item Following the DQN implementation, proceeding with the development and fine-tuning of the PPO agent, including enhanced reward shaping and hyperparameter optimization.
\item Expanding the trading environment to model more realistic market conditions such as transaction costs, slippage, and possibly varying liquidity.
\item Performing ablation studies to isolate the effects of PPO-specific components like clipping, value function loss weighting, and entropy regularization.
\item Enhancing visualization tools to track detailed performance metrics such as Sharpe ratio, maximum drawdown, and other risk-adjusted return measures.
\end{itemize}

\noindent These steps will establish a robust foundation for evaluating advanced reinforcement learning algorithms in the trading domain and inform subsequent research and publication efforts.

\newpage

\chapter{Week 11}
\minitoc

\newpage
\blankpage

\newpage

\section{Deep Q-Learning}

\subsection{Problem Formulation}
\indent We cast single-asset trading as a finite-horizon Markov Decision Process (MDP) on time-indexed observations $\{o_t\}_{t=1}^T$. At each discrete time step $t$, the agent observes a state $s_t \in \mathcal{S}$ (Price, Close, High, Low,Open and Volume), takes an action $a_t \in \mathcal{A}$(Buy, Old or Sell), receives a reward $r_t \in \mathbb{R}$.

\paragraph{State.}
To incorporate temporal context, we define
\[
s_t = \big[\phi(o_{t-w+1}),\,\ldots,\,\phi(o_{t})\big] \in \mathbb{R}^{w\times d},
\]
where $w$ is a rolling window length and $\phi(\cdot)$ is a feature map including (but not limited to): log-returns, rolling z-scores, realized volatility estimates, microstructure features (e.g., imbalance), and technical indicators. To avoid lookahead bias, all rolling statistics are computed using past data only and fit on the training split.

\paragraph{Action Space.}
We consider a discrete policy with position control and an optional \emph{hold} action:
\[
\mathcal{A}=\{-K,\ldots,-1,0,1,\ldots,K\},
\]
where $a_t$ represents the target position (short to long) in normalized units subject to a position limit $|a_t|\leq K$. An alternative is the set $\{\textsc{Short}, \textsc{Flat}, \textsc{Long}\}$ with an additional \textsc{Hold} that preserves the prior position.

\paragraph{Reward.}
Let $P_t$ be the execution price proxy, $\Delta P_{t+1}=P_{t+1}-P_t$, and $\Delta a_t=a_t-a_{t-1}$. A trading-aware reward that accounts for P\&L, costs, and risk is
\begin{equation}
\label{eq:reward}
r_t \;=\; a_t \cdot \Delta P_{t+1}
\;-\; c\,|\Delta a_t|
\;-\; \lambda_{\mathrm{risk}}\,\hat{\sigma}_{t}^2
\;-\; \lambda_{\mathrm{dd}}\,\max(0, \mathrm{DD}_t - \mathrm{DD}_{\max}),
\end{equation}
where $c$ is per-unit transaction/slippage cost, $\hat{\sigma}_t^2$ is an ex-ante volatility estimate, and $\mathrm{DD}_t$ is running drawdown. The last term softly penalizes breach of a drawdown budget $\mathrm{DD}_{\max}$.

\subsection{Deep Q-Network (DQN)}
DQN approximates the action-value function $Q^\star(s,a)$ with a neural network $Q_\theta(s,a)$ trained to minimize the temporal-difference (TD) error using a replay buffer $\mathcal{D}$ and a target network $Q_{\bar{\theta}}$.

\paragraph{Bellman Target.}
For terminal indicator $d_{t+1}\in\{0,1\}$,
\begin{align}
y_t^{\text{DQN}} &= r_t + \gamma (1-d_{t+1}) \max_{a'} Q_{\bar{\theta}}(s_{t+1}, a'), \\
\mathcal{L}(\theta) &= \mathbb{E}_{(s_t,a_t,r_t,s_{t+1},d_{t+1})\sim \mathcal{D}}
\left[ \ell_\kappa\big(y_t - Q_{\theta}(s_t,a_t)\big) \right],
\end{align}
where $\ell_\kappa(\cdot)$ is the Huber loss for robustness to heavy-tailed TD errors. The discount $\gamma\in(0,1)$ should reflect the trading horizon (e.g., $\gamma\in[0.95,0.999]$ for intraday vs. swing horizons).

\paragraph{Double DQN.}
To reduce overestimation, we use Double DQN with decoupled action selection/evaluation:
\begin{equation}
\label{eq:double-dqn}
y_t^{\text{DDQN}} = r_t + \gamma (1-d_{t+1})\;
Q_{\bar{\theta}}\!\left(s_{t+1}, \arg\max_{a'} Q_\theta(s_{t+1},a')\right).
\end{equation}

\paragraph{Architecture.}
For time-series, $Q_\theta$ can be (i) a 1D-CNN over the window $w$, (ii) an LSTM/GRU encoder with the last hidden state feeding an MLP head that outputs $|\mathcal{A}|$ Q-values, or (iii) a Transformer encoder for long-range dependencies. Layer normalization and dropout mitigate non-stationarity; \texttt{ReLU}/\texttt{GELU} activations are typical.

\paragraph{Stabilization.}
We employ (i) target network smoothing with soft updates
$\bar{\theta} \leftarrow \tau \theta + (1-\tau)\bar{\theta}$, $\tau\!\ll\!1$,
(ii) prioritized replay with sampling probability $\propto | \delta_t |^\alpha$ and importance weights, and (iii) action masking when risk or inventory limits are hit.

\subsection{Exploration and Constraints}
We use $\epsilon$-greedy with linear or cosine decay from $\epsilon_{\max}$ to $\epsilon_{\min}$; in practice, \emph{noisy layers} can replace explicit $\epsilon$ for state-dependent exploration. Trading-specific constraints (max position $K$, max turnover, exposure to news embargo windows) are enforced via an action mask $m_t(a)\in\{0,1\}$ and
\[
a_t \;=\; \arg\max_{a \in \mathcal{A}: m_t(a)=1} Q_\theta(s_t,a).
\]

\subsection{Training Algorithm}

\subsection{Practical Considerations for Time-Series}
\paragraph{Data Splitting \& Leakage.}
Use chronological splits and \emph{walk-forward} evaluation: rolling train/validation windows with a held-out test period. All preprocessing (scalers, PCA, feature selection) must be fit on training only and applied forward.

\paragraph{Stationarity \& Regimes.}
Markets are non-stationary; periodic target network updates ($\tau$) and shorter replay horizons help. Consider re-training or fine-tuning across regimes and adding a \emph{regime feature} (e.g., volatility state) to $s_t$.

\paragraph{Costs \& Slippage.}
Model costs explicitly in $r_t$ and optionally inject execution noise during training to bridge the sim-to-real gap. Limit turnover via the $|\Delta a_t|$ penalty.

\paragraph{Risk Controls.}
In addition to reward penalties, enforce hard caps: max position $K$, max leverage, and a circuit breaker when rolling drawdown exceeds $\mathrm{DD}_{\max}$.

\subsection{Evaluation Metrics}
Let $\{R_t\}$ be realized returns from the executed strategy. Report:
\begin{itemize}
  \item Annualized Sharpe: $\displaystyle \mathrm{SR}=\frac{\sqrt{A}\,\mathbb{E}[R_t]}{\mathrm{Std}[R_t]}$ with $A$ the periods-per-year factor.
  \item Sortino, Calmar, hit ratio, average trade, turnover, max drawdown, and profit factor.
  \item Stability: rolling SR and drawdown; sensitivity to cost $c$; ablations (no costs, no risk term, no mask).
\end{itemize}
Always compare to baselines (buy-and-hold, momentum/mean-reversion heuristics) and include a \emph{purged, embargoed} cross-validation if you use overlapping windows.

\subsection{Model Variants (Optional)}
\begin{itemize}
  \item \textbf{Dueling DQN:} Decompose $Q_\theta(s,a)=V_\theta(s)+A_\theta(s,a)-\frac{1}{|\mathcal{A}|}\sum_{a'}A_\theta(s,a')$ to stabilize value estimation.
  \item \textbf{N-step Returns:} Replace $y$ with $n$-step target $r_t+\gamma r_{t+1}+\cdots+\gamma^{n-1} r_{t+n-1}+\gamma^n \max_{a'} Q_{\bar{\theta}}(s_{t+n},a')$.
  \item \textbf{Distributional RL:} Learn the return distribution $Z(s,a)$ for better risk-sensitive control.
  \item \textbf{Noisy Nets:} Parameterized noise in linear layers for exploration without $\epsilon$-schedules.
\end{itemize}

\subsection{Reference Hyperparameters (Typical Ranges)}
\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Typical value \\ \midrule
Window length $w$ & 32--256 steps \\
Discount $\gamma$ & 0.95--0.999 \\
Optimizer / LR & Adam, $10^{-4}$ to $3\cdot 10^{-4}$ \\
Batch size $B$ & 64--256 \\
Replay size $|\mathcal{D}|$ & $10^5$--$10^6$ \\
Target update & soft $\tau \in [10^{-3}, 10^{-2}]$ (or hard every 1--5k steps) \\
$\epsilon$ schedule & from 1.0 to 0.05 over $10^5$ steps (or NoisyNets) \\
Cost $c$ & set by venue; stress $\times 2\text{--}4$ for robustness \\
Risk weights & $\lambda_{\mathrm{risk}}, \lambda_{\mathrm{dd}}$ via grid search on validation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}
DQN assumes a stationary $Q^\star$ and Markovian dynamics, both often violated in markets. Performance can degrade under regime shifts, changing costs/liquidity, or adversarial feedback. Robustness checks (stress costs, volatility spikes, delayed fills) and conservative deployment (small capital, shadow trading) are essential.

\section{Transformer-Based Deep Q-Learning for Time-Series Trading}
\label{sec:transformer-dqn-trading}

\subsection{Problem Formulation}
We frame single-asset trading as a finite-horizon Markov Decision Process (MDP) over price and feature sequences $\{o_t\}_{t=1}^T$. At each time $t$, the agent observes a state $s_t \in \mathcal{S}$, selects an action $a_t \in \mathcal{A}$, receives reward $r_t$, and transitions to $s_{t+1}$. 

\paragraph{State.}  
The state is a sequence of past observations:
\[
s_t = \big[\phi(o_{t-w+1}), \dots, \phi(o_t)\big] \in \mathbb{R}^{w \times d},
\]
where $w$ is the window size, $d$ the feature dimension, and $\phi(\cdot)$ includes log-returns, volatility, and other technical indicators.

\paragraph{Action Space.}  
We use a discrete set of position actions:
\[
\mathcal{A} = \{-K, \dots, -1, 0, 1, \dots, K\},
\]
representing target positions (short to long) subject to max position $K$.

\paragraph{Reward.}  
The reward accounts for P\&L, trading costs, and risk:
\[
r_t = a_t \cdot \Delta P_{t+1} - c |\Delta a_t| - \lambda_{\mathrm{risk}} \hat{\sigma}_t^2 - \lambda_{\mathrm{dd}} \max(0, \mathrm{DD}_t - \mathrm{DD}_{\max}).
\]

\subsection{Transformer Q-Network}
Instead of a traditional CNN/LSTM, we use a Transformer encoder to model long-range dependencies in time-series. The network $Q_\theta(s_t, a_t)$ is parameterized as:

\begin{itemize}
    \item Input: sequence of feature vectors $s_t$.
    \item Positional encodings added to preserve temporal order.
    \item Stacked Transformer encoder layers with multi-head attention.
    \item Final MLP head outputs $|\mathcal{A}|$ Q-values.
\end{itemize}

Formally, let $\text{Transformer}_\theta(\cdot)$ denote the output embedding for the last token:
\[
Q_\theta(s_t, a) = \text{MLP}_\theta\big(\text{Transformer}_\theta(s_t)\big)_a.
\]

\subsection{Training with Double DQN Targets}
The Transformer Q-network is trained using Double DQN targets:
\[
y_t = r_t + \gamma (1 - d_{t+1}) Q_{\bar{\theta}}\Big(s_{t+1}, \arg\max_{a'} Q_\theta(s_{t+1}, a')\Big),
\]
where $\bar{\theta}$ is a target network updated softly: $\bar{\theta} \gets \tau \theta + (1-\tau)\bar{\theta}$.

The loss is the Huber loss over a replay buffer $\mathcal{D}$:
\[
\mathcal{L}(\theta) = \mathbb{E}_{(s_t,a_t,r_t,s_{t+1},d_{t+1}) \sim \mathcal{D}} \big[ \ell_\kappa(y_t - Q_\theta(s_t, a_t)) \big].
\]

\subsection{Exploration and Constraints}
\begin{itemize}
    \item \textbf{Exploration:} $\epsilon$-greedy or parameter noise (NoisyNet layers) in the MLP head.
    \item \textbf{Constraints:} Action masks enforce max position, turnover, and risk limits.
\end{itemize}

\subsection{Practical Considerations}
\begin{itemize}
    \item \textbf{Data Splitting:} Walk-forward evaluation to prevent lookahead bias.
    \item \textbf{Stationarity:} Transformers can capture longer temporal dependencies but may still require retraining on regime shifts.
    \item \textbf{Costs \& Slippage:} Include in the reward function to improve robustness.
    \item \textbf{Hyperparameters:} Window length $w$, number of Transformer layers, number of attention heads, hidden dimensions, learning rate, batch size, replay buffer size.
\end{itemize}

\subsection{Evaluation Metrics}
Use the same trading metrics as before: Sharpe, Sortino, maximum drawdown, hit ratio, turnover, and profit factor. Compare against buy-and-hold and heuristic baselines.

\subsection{Remarks}
Replacing the RNN/CNN with a Transformer enables the agent to capture longer-range dependencies in time-series, which is beneficial for assets with complex temporal patterns or irregular cycles. Care must be taken to limit overfitting due to increased model capacity.



\begin{algorithm}[H]
\caption{Deep Q-Learning Training}
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}




\newpage

\chapter{Week 12}
\minitoc

\newpage
\blankpage

\newpage


\section{Deep Q-Learning}

\subsection{Deep Q-Learning introduction}

\indent In order to approximate optimal decision-making in complex environments, we employ Deep Q-Learning, an extension of Q-Learning that integrates state discretization with function approximation techniques. Unlike traditional tabular Q-Learning, which directly maintains a Q-table over discrete states, Deep Q-Learning is capable of handling continuous or high-dimensional state spaces by discretizing them into manageable bins. The algorithm balances exploration and exploitation using an $\epsilon$-greedy strategy with exponential decay, ensuring sufficient exploration during early episodes while gradually converging toward exploitation of learned policies. At each training step, the Q-table is updated via the Bellman equation, incorporating observed rewards and estimated future returns. The following pseudocode outlines the full training procedure.

\vfill

\begin{algorithm}[H]
\caption{Deep Q-Learning Training}
\KwIn{Environment $env$, data frame $df$, training size $train\_size$, episode $N$}
\KwOut{Trained Q-table $Q$}

Split data: $df_{train} \leftarrow$ $train\_size$ of $df$\;

Calculate bins for discretization : $bins \leftarrow$ compute bins from $df_{train}$\;

Initialize Q-table

\For{$episode \leftarrow 1$ \KwTo $N$}{
    Exploration rate : $\epsilon \leftarrow \epsilon_{min} + (\epsilon_{max} - \epsilon_{min}) \cdot \exp(-decay\_rate \times episode)$\;
    
    Reset environment : $state_{cont} \leftarrow env.reset()$\;
    
    Discretize initial state : $state_{disc} \leftarrow discretize(state_{cont}, bins)$\;
    
    Convert to index : $state_{idx} \leftarrow state\_to\_index(state_{disc}, bins)$\;
    
    \For{$step \leftarrow 1$ \KwTo $max\_steps$}{
        
        Choose action $a$ using epsilon-greedy policy :
        \[
        a \leftarrow
        \begin{cases}
            \arg\max_{a'} Q[state_{idx}, a'] & \text{with probability } 1 - \epsilon \\
            \text{random valid action} & \text{with probability } \epsilon
        \end{cases}
        \]
        
        Take action : $(next_{state_{cont}}, r, done, info) \leftarrow env.step(a)$\;
        
        Discretize next state : $next_{state_{disc}} \leftarrow discretize(next_{state_{cont}}, bins)$\;
        
        Convert to index : $next_{state_{idx}} \leftarrow state\_to\_index(next_{state_{disc}}, bins)$\;
        
        Update Q-value :$Q[state_{idx}, a] \leftarrow Q[state_{idx}, a] + \alpha \left( r + \gamma \max_{a'} Q[next_{state_{idx}}, a'] - Q[state_{idx}, a] \right)$\;
        
        $state_{idx} \leftarrow next_{state_{idx}}$\;
        
        \If{$done$}{
            \textbf{break}\;
        }
    }
}

\Return $Q$
\end{algorithm}

\vfill

\newpage

\subsection{Deep Q-Learning Process}

\indent To implement this method, we will use the same Q-Learning algorithm object and change during the training to a Neural Network. As a first example we will use a MLP.

\begin{center}
\begin{tikzpicture}[
    node distance=2.5cm,
    box/.style={
        draw,
        rounded corners,
        minimum width=2.5cm,
        minimum height=1cm,
        align=center,
        font=\small,
        fill=blue!10
    },
    arrow/.style={
        -{Stealth[length=2mm]},
        thick
    }
]

% Nodes
\node[box] (state) {State};
\node[box, right=of state] (dqn) {DeepQ*Learning};
\node[box, above right=1.5cm and 2.5cm of dqn] (sell) {Sell};
\node[box, right=of dqn] (hold) {Hold};
\node[box, below right=1.5cm and 2.5cm of dqn] (buy) {Buy};

% Connectors and Labels
\coordinate (connect) at ($(dqn)!0.5!(hold)$);
\draw[arrow] (state) -- (dqn.west);
\draw[arrow] (connect) |- (sell);
\draw[arrow] (connect) |- (buy);
\draw[arrow] (dqn.east) -- (hold);

\end{tikzpicture}
\end{center}

\subsection{MLP layer}

\indent The Multi-Layer Perceptron (MLP) layer serves as the core of the neural network, responsible for learning and transforming input data through a series of connected, dense layers. It is composed of multiple fully-connected layers, each followed by a non-linear activation function. This non-linearity is crucial as it allows the network to model complex, non-linear relationships in the data that a simple linear model could not capture.\par


\indent The \texttt{MLP} class is defined as a PyTorch module, which is a standard approach for building neural network components. The constructor, \texttt{\_\_init\_\_}, initializes the network's architecture. It takes three key arguments: \texttt{input\_dim} (the dimensionality of the input data), \texttt{output\_dim} (the number of output units), and \texttt{hidden\_dims} (a tuple specifying the number of units in each hidden layer, which defaults to \texttt{(128, 128)}). The implementation uses a loop to dynamically build the hidden layers. For each dimension specified in \texttt{hidden\_dims}, it adds a \texttt{nn.Linear} layer (a fully-connected layer) followed by a \texttt{nn.ReLU} activation function. The Rectified Linear Unit (ReLU) is chosen for its computational efficiency and its effectiveness in preventing the vanishing gradient problem.\par

\indent After the hidden layers are constructed, a final \texttt{nn.Linear} layer is added. This last layer maps the output of the final hidden layer to the desired \texttt{output\_dim} of the network. The entire sequence of layers is then encapsulated into a single \texttt{nn.Sequential} container, which ensures that the data will be passed through the layers in the correct order during the forward pass.\par

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type}\\
        \midrule
        \texttt{nn.Linear} & Linear Layer \\
        \texttt{nn.ReLU} & Activation Function \\
        \texttt{nn.Sequential} & Container \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the MLP layers}
    \label{tab:mlp_layers}
\end{table}

\newpage


\indent After a 2-hour training period, the model's performance was evaluated on a test dataset. The results are visualized in the following plot :

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/MLP_result/QLearning_BTC-USD_2014.png}
    \caption{Performance of the Q-Learning Model on a BTC-USD test set}
    \label{fig:performance_plot}
\end{figure}

\indent The plot demonstrates that the model was able to generate positive returns, indicating a degree of success in the training. However, it is critical to note that this test was conducted on data from an overall uptrend market. This suggests the model's profitability may be a result of the market's general direction rather than its ability to make strategic decisions in both bullish and bearish conditions.\par

\indent A closer inspection of the training log and the trading history reveals that the model made a significant number of "illegal moves." These unauthorized actions, which violate the predefined trading constraints or rules of the environment, are a clear indication of a failure in the model's policy and training process. This result suggests that while the model found a way to profit, it did so by exploiting an oversight in the environment or by not correctly learning the full set of trading rules. Further work is required to correct these behaviors and ensure the model operates within the defined constraints, leading to a more robust and generalizable trading policy.\par


\indent To further analyze the model's performance across different market conditions, we present additional trading simulations on various stock datasets :

\vfill

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/MLP_result/QLearning_AAPL_2010.png}
    \caption{Performance on Apple (AAPL) data from 2010.}
\end{figure}

\vfill

\newpage

\begin{figure}[h]
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_ATOS_2017.png}
        \caption{Performance on ATOS data from 2017.}
        \label{fig:performance_atos}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_O_2016.png}
        \caption{Performance on Realty Income (O) data from 2016.}
        \label{fig:performance_o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_RNO_2016.png}
        \caption{Performance on Renault (RNO) data from 2016.}
        \label{fig:performance_rno}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../img/MLP_result/QLearning_TSLA_2019.png}
        \caption{Performance on Tesla (TSLA) data from 2019.}
        \label{fig:performance_tsla}
    \end{subfigure}

    \caption{Model performance across various market conditions.}
    \label{fig:performance_plots_all}
\end{figure}

\indent The plots demonstrate that the model was able to generate positive returns, indicating a degree of success in the training. However, it is critical to note that the test on the BTC-USD data was conducted on an overall uptrend market. This suggests the model's profitability may be a result of the market's general direction rather than its ability to make strategic decisions in both bullish and bearish conditions.\par

\noindent A closer inspection of the training log and the trading history reveals that the model made a significant number of "illegal moves." These unauthorized actions, which violate the predefined trading constraints or rules of the environment, are a clear indication of a failure in the model's policy and training process. This result suggests that while the model found a way to profit, it did so by exploiting an oversight in the environment or by not correctly learning the full set of trading rules. Further work is required to correct these behaviors and ensure the model operates within the defined constraints, leading to a more robust and generalizable trading policy.


\subsection{Decision Transformer Layer}

\indent Unlike a traditional Multi-Layer Perceptron (MLP) that processes a single state at a time, the DecisionTransformerQ layer is designed to handle sequential data, leveraging the powerful architecture of a transformer. This approach frames reinforcement learning as a sequence modeling problem, where the model learns to predict future actions based on a history of past states and desired returns.\par

\noindent The \texttt{DecisionTransformerQ} class is built upon a pre-trained transformer model from Hugging Face's library, which serves as the core `backbone`. The constructor, \texttt{\_\_init\_\_}, initializes a configuration for the transformer and loads the corresponding model (`DecisionTransformerGPT2Model`). This approach allows the network to benefit from the pre-trained weights, which are already effective at capturing complex patterns in sequential data.\par

\noindent Before the input is fed into the transformer, it passes through a `self.input\_proj` linear layer. This layer's purpose is to project the raw input data, which has a dimensionality of \texttt{input\_dim}, into an embedding space that matches the transformer's hidden size. This ensures the input is correctly formatted for the transformer architecture.\par

\noindent The \texttt{forward} method outlines the data flow. First, it ensures the input tensor \texttt{x} is in the correct shape for the transformer (a 3D tensor representing a batch, sequence of steps, and features). The input is then passed to the `self.input\_proj` layer for projection. The core of the computation happens when the projected data is fed into the transformer `backbone`. The transformer processes the entire sequence and its output's `last\_hidden\_state` is used. This final hidden state is a rich representation that has attended to the entire sequence history, making it ideal for the final decision. Finally, the `self.q\_head` linear layer takes this comprehensive representation and projects it to the desired \texttt{output\_dim}, providing the predicted Q-values.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type} \\
        \midrule
        \texttt{DecisionTransformerGPT2Model} & Transformer Backbone \\
        \texttt{nn.Linear} (Input) & Linear Layer \\
        \texttt{nn.Linear} (Q-Head) & Linear Layer \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the Decision Transformer layers}
\end{table}


\indent After a 4-hour training period, the model's performance was evaluated on a test dataset. The results are visualized in the following plot:

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{../img/GPT_transformer_results/DeepQLearning_O_2016_v2.png}
    \caption{Performance of the Deep Q-Learning Model on Realty Income (O) data from 2016.}
    \label{fig:deep_q_performance}
\end{figure}

\indent This model achieved its best results on the Realty Income (O) stock during both training and testing. While the model was able to generate positive returns, further analysis is required because the stock's volatility is high due to a large number of trades. The next step will be to add broker fees to the model to limit the number of trades and make the results more representative of real-world trading conditions.\par

\newpage

\chapter{Week 13}
\minitoc

\newpage
\blankpage

\newpage


\section{Deep Q-Learning}
\subsection{Decision Transformer Layer}

\indent Unlike a traditional Multi-Layer Perceptron (MLP) that processes a single state at a time, the DecisionTransformerQ layer is designed to handle sequential data, leveraging the powerful architecture of a transformer. This approach frames reinforcement learning as a sequence modeling problem, where the model learns to predict future actions based on a history of past states and desired returns.\par

\noindent The \texttt{DecisionTransformerQ} class is built upon a pre-trained transformer model from Hugging Face's library, which serves as the core `backbone`. The constructor, \texttt{\_\_init\_\_}, initializes a configuration for the transformer and loads the corresponding model (`DecisionTransformerGPT2Model`). This approach allows the network to benefit from the pre-trained weights, which are already effective at capturing complex patterns in sequential data.\par

\noindent Before the input is fed into the transformer, it passes through a `self.input\_proj` linear layer. This layer's purpose is to project the raw input data, which has a dimensionality of \texttt{input\_dim}, into an embedding space that matches the transformer's hidden size. This ensures the input is correctly formatted for the transformer architecture.\par

\noindent The \texttt{forward} method outlines the data flow. First, it ensures the input tensor \texttt{x} is in the correct shape for the transformer (a 3D tensor representing a batch, sequence of steps, and features). The input is then passed to the `self.input\_proj` layer for projection. The core of the computation happens when the projected data is fed into the transformer `backbone`. The transformer processes the entire sequence and its output's `last\_hidden\_state` is used. This final hidden state is a rich representation that has attended to the entire sequence history, making it ideal for the final decision. Finally, the `self.q\_head` linear layer takes this comprehensive representation and projects it to the desired \texttt{output\_dim}, providing the predicted Q-values.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l l|}
        \toprule
        \textbf{Layer} & \textbf{Type} \\
        \midrule
        \texttt{DecisionTransformerGPT2Model} & Transformer Backbone \\
        \texttt{nn.Linear} (Input) & Linear Layer \\
        \texttt{nn.Linear} (Q-Head) & Linear Layer \\
        \bottomrule
    \end{tabular}
    \caption{Explanation of the Decision Transformer layers}
    \label{tab:decision_transformer_layers}
\end{table}

\subsection{GPT2 Transformer description}

\vspace{0.5cm}

\begin{center}
\begin{tikzpicture}[
    scale=0.70,
    transform shape,
    node distance=0.5cm,
    box/.style={
        draw,
        rounded corners,
        minimum width=2.5cm,
        minimum height=1cm,
        align=center,
        font=\small,
        fill=blue!10
    },
    arrow/.style={
        -{Stealth[length=2mm]},
        thick
    }
]

% Nodes
\node[box, fill=blue!30] (input) {INPUT};
\node[box, right=of input, scale=0.8] (embedding) {Embedding};
\node[box, right=0.2cm of embedding, scale=0.8] (drop) {Drop Out};
\node[box, right=of drop, fill=gray, scale=1.4] (attention) {GPT Attention};
\node[box, right=of attention, scale = 0.8] (normalization1) {Normalization};
\node[box, right=of normalization1, fill=gray, scale=1.4] (transformer) {GPT Transformer};
\node[box, right=of transformer, scale=0.8] (normalization2) {Normalization};
\node[box, right=of normalization2, fill=blue!30] (out) {OUTPUT};

% Connectors and Labels
\draw[arrow] (input) -- (embedding);
\draw[arrow] (embedding) -- (drop);
\draw[arrow] (drop) -- (attention);
\draw[arrow] (attention) -- (normalization1);
\draw[arrow] (normalization1) -- (transformer);
\draw[arrow] (transformer) -- (normalization2);
\draw[arrow] (normalization2) -- (out);

\end{tikzpicture}
\end{center}

\noindent As we can see, the GPT2 transformer is a basic transformer, however the good performance of this particular transformer is that the model has been created using the GPT2 opensource transformer available on hugging face. Here are some results of the model on different stocks.

\newpage

\vfill
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_AAPL_2010.png}
    \caption{Performance of the Transofrmer Model on Apple data from 2010.}
\end{figure}
\vfill
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_O_2016_v2.png}
    \caption{Performance of the Transofrmer Model on Realty income data from 2016.}
\end{figure}
\vfill
\newpage
\vfill
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_renault_2016.png}
    \caption{Performance of the Transofrmer Model on Renault data from 2016.}
\end{figure}
\vfill
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/GPT_transformer_results/DeepQLearning_TSLA_2019.png}
    \caption{Performance of the Transofrmer Model on Tesla data from 2019.}
\end{figure}
\vfill

\indent As we can see, there is a too many trades that are made during the training and testing part, this is an issue caused by the penalty of the model ; there is no penalty on the number of trade the bot will make, this suggest that we need to find a way to make the bot understand that a trade should be made wiser. For that we suggest to add the broker's commission for a buy or sell action. Let's take a 10\% commission which make the bot considering the number of trade made.

\vfill

\newpage

\subsection{Result for QLearning (with commission)}

\indent We changed the environment behavior to provide a 10 \% commission for each buy and sell movements. Also we changed the reward to provide only the difference between the initial wallet et the current portfolio so that the agent learn also on the market behavior directly without any outside penalties that could makes him learn false dynamic. The problem here could be that the agent does not learn about the rules of the set of action. For example, the issue here could be that the model do lot of illegals moves such as buying twice in a row, if that happen, we will change the reward back with penalties. Here are the results on the QLearning agent :

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_AAPL_2010_commission.png}
    \caption{Performance of the QLearning Model on Apple data from 2010 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_BTC-USD_2014_commission.png}
    \caption{Performance of the QLearning Model on Bitcoin data from 2014 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{../img/expriment_on_commission/QLearning_TSLA_2019_commission.png}
    \caption{Performance of the QLearning Model on Tesla data from 2019 with commission}
\end{figure}

\noindent As we can see the number of trades reduced, however the performance is still poor since it is still following the trend of the actual stock. This last observation could be explained by the fact that QLearning is a basic Machine Learning algorithm.


\subsection{Result for DeepQLearning - GPT Transformer (with commission)}

\indent We did the same experiment on the transformer model. However this model takes a while to train, we could not have a lot of output results so we will analyze only two experiments :

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{../img/expriment_on_commission/DeepQLearning_O_2016_commission.png}
    \caption{Performance of the Transformer Model on Real income data from 2016 with commission}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{../img/expriment_on_commission/DeepQLearning_TSLA_2019_commission.png}
    \caption{Performance of the Transformer Model on Tesla data from 2019 with commission}
\end{figure}

\noindent As we can see the performance on the training part of Real income is really good, the agent seems to understand the dynamic of the stock as a comparison, the current market made 80\% profit on this period while our model made 98\%, which means that on the training part our model get very well the dynamic aspect of the market. However we can see the limitation of the model during the testing part where our model made -2\% and the current market lost 6\% which indicates that our model also outperform the market but made us lose money still.\par

\noindent On another hand, on the Tesla market, the agent has completely been broke on his behaviour. The difference with the previous agent is that the environement is giving a reward based on the difference in between the initial wallet and the actual portfolio, which later in a report, is suggested as a better reward based for a trading bot using transformer. This could also be an issue from the transofrmer model itself (bad intergation of the model / bad purpose of the model), we will analyze this issue later.

\newpage

\section{Paper : Transformers in Reinforcement Learning : A survey}


\indent Transformers have emerged as a dominant architecture in modern machine learning, revolutionizing natural language processing, computer vision, and a growing number of other domains. Their ability to model long-range dependencies, capture contextual information, and scale effectively has prompted increasing interest in applying them to reinforcement learning (RL). In RL, agents must learn sequential decision-making policies under uncertainty, a setting that aligns naturally with the sequence modeling capabilities of transformers. Over the past few years, researchers have proposed a variety of transformer-based methods for RL tasks, ranging from policy optimization and value estimation to model-based approaches.\par

\noindent This survey, written in 2023 by Pranav Agarwal (cole de Technologie Suprieure/Mila, Canada), Aamer Abdul Rahman (cole de Technologie Suprieure/Mila, Canada), Pierre-Luc St-Charles (Mila, Applied ML Research Team, Canada), Simon J.D. Prince (University of Bath, United Kingdom) and Samira Ebrahimi Kahou (cole de Technologie Suprieure/Mila/CIFAR, Canada), provides a comprehensive overview of this rapidly developing research area. The paper reviews how transformers have been adapted for reinforcement learning, highlights emerging architectures and methodologies, and categorizes them according to their applications. It also discusses empirical results, key challenges such as scalability and sample efficiency, and potential future research directions.\par

\noindent By situating transformer-based RL within the broader landscape of sequence modeling and decision-making, this survey contributes to a deeper understanding of both the opportunities and limitations of applying transformers in reinforcement learning. It serves as a reference point for researchers and practitioners seeking to build on the foundations of this promising intersection of fields. This paper talk about the transformer on many different fields but the interesting part for us is the trading part.

\subsection{Transformer RL in Trading}

\indent Portfolio optimization aims to balance returns and risks when selecting assets but this is difficult due to market volatility and external factors. Reinforcement learning (RL) has been explored to automate trading decisions by learning from historical market data, such as price trends, volumes and sentiment.\par

\noindent Transformers are particularly suited for this task because they can capture both sequential patterns in asset prices and correlations between different assets. It has been introduced that the Relation-Aware Transformer (RAT) for portfolio selection, where the encoder extracts sequential and relational features, and the decoder makes trading decisions (including leverage and short sales). The approach was evaluated on real-world stock and cryptocurrency data, showing competitive performance compared to state-of-the-art portfolio selection methods. The given model would is described as an encoder-decoder transformer like so :

\vspace{1cm}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        scale=0.9,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={
                -{Stealth[length=2mm]},
        thick
        }
        ]
        
        % Nodes
        \node[box, fill=blue!30] (input) {INPUT};
        \node[box, right=of input, scale=0.8] (embedding) {Embedding};
        \node[box, right=of embedding, fill=gray, scale=1.4] (encoder) {Encoder};
        \node[box, right=of encoder, fill=gray, scale=1.4] (decoder) {Decoder};
        \node[box, right=of decoder, scale=0.8] (normalization) {Normalization};
        \node[box, right=of normalization, fill=blue!30] (out) {OUTPUT};
        
        % Connectors and Labels
        \draw[arrow] (input) -- (embedding);
        \draw[arrow] (embedding) -- (encoder);
        \draw[arrow] (encoder) -- (decoder);
        \draw[arrow] (decoder) -- (normalization);
        \draw[arrow] (normalization) -- (out);
        
    \end{tikzpicture}
    \caption{Encoder-Decoder Stack of the transofrmer model}
\end{figure}

\newpage

\noindent Let us now dive into the encoder stack to understand how these sequential and relational features are extracted and represented for reinforcement learning-based portfolio optimization.

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[
        scale=1,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={-{Stealth[length=2mm]}, thick},
            line/.style={thick}
            ]

        % Nodes
        \node[box, fill=blue!30] (input) {INPUT};
        \node[coordinate, above=of input] (junction1) {} ;
        \node[box, above=of junction1,fill=gray ,scale=1.4] (mha) {Multi-Head\\ Attention};
        \node[box, above=of mha, scale=0.9] (AddNorm1) {Add \& Norm};
        \node[coordinate, above=of AddNorm1] (junction2) {} ;
        \node[box, above=of junction2] (ff) {Feed Forward} ;
        \node[box, above=of ff] (AddNorm2) {Add \& Norm} ;
        \node[box, above=of AddNorm2, fill=blue!30] (output) {OUTPUT};

        % Connectors
        \draw[line] (input.north) -- (junction1);
        \draw[arrow] (junction1) -- (mha);
        \draw[arrow] (mha) -- (AddNorm1);
        \draw[arrow] (junction1.west) -- ++(-2.5, 0) |- (AddNorm1.west);
        \draw[arrow] (AddNorm1) -- (junction2);
        \draw[line] (junction2) -- (ff);
        \draw[arrow] (ff) -- (AddNorm2);
        \draw[arrow] (junction2.west) -- ++(-2.5, 0) |- (AddNorm2.west);
        \draw[arrow] (AddNorm2) -- (output);

\end{tikzpicture}
\caption{Encoder stack}
\vspace{-.8cm}
\end{wrapfigure}

\indent In the context of trading and portfolio optimization, the encoder plays a crucial role in capturing the complex patterns present in historical market data. It processes sequences of asset prices, trading volumes and other market indicators to model both short-term trends and long-term dependencies. By leveraging the self-attention mechanism inherent to transformers, the encoder can identify important relationships not only within a single asset's time series but also across multiple assets, effectively capturing correlations that are critical for informed trading decisions.\par

\noindent In the context of a transformer model, we are using the Attention architecture to capture temporal dependencies and correlations in sequential trading data. The attention mechanism allows the model to weigh the importance of past states and actions when predicting future trading decisions, which is particularly useful in financial markets where certain events or trends can have long-term effects. The Multi-Head Attention (MHA) layer enables the model to simultaneously focus on different aspects of the input sequences, such as price trends, trading volumes and market sentiment. By splitting the attention into multiple heads, the network can learn diverse representations of the data, improving its ability to capture complex relationships between assets.\par

\noindent Let's now describe the two main component of these layers which are the Multi-Head Attention layer and the Feed Forward layer. We will also describe the basis of original Attention layer that is included into the Multi-Head Attention :

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        scale=0.7,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
            },
            arrow/.style={-{Stealth[length=2mm]}, thick},
            line/.style={thick}
            ]

        % Nodes
        \node[box, fill=blue!30, minimum width=12cm] (input) {INPUT};

        %Attention head #1
        \node[box, above=2cm of input.west] (linear1) {Linear};
        \node[box, above=of linear1, fill=orange!80, scale=1.4] (attention1) {\\Attention \#1\\ \\ $softmax\left( \frac{QK^T}{\sqrt{d_k}} \right)V$};

        %Dot x8
        \node[above=4cm of input, scale=1.4] (dots) {$\cdots\vspace{3mm}\times 8$};

        %Attention head #8
        \node[box, above=2cm of input.east] (linear8) {Linear};
        \node[box, above=of linear8, fill=orange!80, scale=1.4] (attention8) {\\Attention \#8\\ \\ $softmax\left( \frac{QK^T}{\sqrt{d_k}} \right)V$};

        % Output
        \node[box, above=7cm of input, minimum width=12cm] (concat) {Concat};
        \node[box, above=of concat] (linear3) {Linear};
        \node[box, above=of linear3, fill=blue!30] (output) {OUTPUT};

        % ========================================================================================

        % Connectors
        \draw[arrow]  ([xshift=-20mm]input.north)  -- ++(0, 0.5) -|  (linear1) ;
        \draw[dashed] ([yshift=+5mm]input.north) -- ++(-0.5,0.5)  --   ([xshift=-5.3mm ,yshift=-8mm]dots.south) ;
        \draw[dashed] (input.north)  --   ([yshift=-10mm]dots.south) ;
        \draw[dashed] ([yshift=+5mm]input.north) -- ++(0.5,0.5)  --   ([xshift=+5.3mm ,yshift=-8mm]dots.south) ;
        \draw[arrow]  ([xshift=20mm]input.north)  -- ++(0, 0.5) -|  (linear8) ;
    
        %Attention head #1
        \draw[arrow] ([xshift=5mm]linear1.north west)  --  ([xshift=15mm]attention1.south west) node[midway, right] {$Q_1$};
        \draw[arrow] (linear1.north)                   --  (attention1) node[midway, right] {$H_1$};
        \draw[arrow] ([xshift=-5mm]linear1.north east) --  ([xshift=-15mm]attention1.south east) node[midway, right] {$V_1$};
        \draw[arrow] (attention1.north)                -- ++(0, 0.5) -|  ([xshift=20mm]concat.south west) ;
    
        %Dot x8
        \draw[dashed] ([xshift=-5.3mm, yshift=+8mm]dots.north) -- ([xshift=-5.3mm]concat.south) ;
        \draw[dashed] ([yshift=+10mm]dots.north)  --   (concat) ;
        \draw[dashed] ([xshift=+5.3mm ,yshift=+8mm]dots.north) -- ([xshift=+5.3mm]concat.south) ;
    
        %Attention head #2
        \draw[arrow] ([xshift=5mm]linear8.north west)  --  ([xshift=15mm]attention8.south west) node[midway, right] {$Q_8$};
        \draw[arrow] (linear8.north)                   --  (attention8) node[midway, right] {$H_8$};
        \draw[arrow] ([xshift=-5mm]linear8.north east) --  ([xshift=-15mm]attention8.south east) node[midway, right] {$V_8$};
        \draw[arrow] (attention8.north)                -- ++(0, 0.5) -|  ([xshift=-20mm]concat.south east) ;


        \draw[arrow] (concat)                          --  (linear3) ;
        \draw[arrow] (linear3)                         --  (output) ;

\end{tikzpicture}
\caption{Multi-Head Attention stack (x8 heads)}
\vspace{-.8cm}
\end{figure}

\newpage

\vspace{2cm}

\begin{figure}[h]
    \centering
    \begin{minipage}{.45\linewidth}
        \centering
        \begin{tikzpicture}[
            scale=1,
            transform shape,
            node distance=0.5cm,
            box/.style={
                draw,
                rounded corners,
                minimum width=3cm,
                minimum height=1cm,
                align=center,
                font=\small,
                fill=blue!10
                },
                arrow/.style={-{Stealth[length=2mm]}, thick},
                line/.style={thick}
                ]

            % Nodes
            \node[box, fill=blue!30] (input) {INPUT};
            \node[box, above=of input.north, minimum width=5cm] (linear) {Linear};
            \node[box, above=of linear.north, anchor=south west, minimum width=1cm] (q) at (linear.north west) {Q};
            \node[box, above=of linear.north, minimum width = 1cm]      (k) {K};
            \node[box, above=of linear.north, anchor=south east, minimum width=1cm] (v) at (linear.north east) {V};
            \node[box, above=of q] (matmul) at ($(q)!0.5!(k) + (0,0.5)$) {matmul};
            \node[box, above=of matmul] (scale) {scale};
            \node[box, above=of scale] (softmax) {softmax};
            \node[box, above=8cm of input, minimum width=5cm] (matmulOut) {matmul};
            \node[box, above=of matmulOut, fill=blue!30] (output) {OUTPUT};

            % Connectors
            \draw[arrow] (input.north) -- (linear);
            \draw[arrow] ([xshift=5mm]linear.north west)  -- (q);
            \draw[arrow] (linear.north) -- (k);
            \draw[arrow] ([xshift=-5mm]linear.north east) -- (v);
            \draw[arrow] (q.north)     -- ([xshift=5mm]matmul.south west);
            \draw[arrow] (k.north)     -- ([xshift=-5mm]matmul.south east);
            \draw[arrow] (matmul)      -- (scale);
            \draw[arrow] (scale)       -- (softmax);
            \draw[arrow] (softmax)     -- ([xshift=-10mm]matmulOut.south);
            \draw[arrow] (v)           -- ([xshift=-5mm]matmulOut.south east);
            \draw[arrow] (matmulOut)   -- (output);

        \end{tikzpicture}
        \caption{Attention stack}
    \end{minipage}\hspace{.5cm}
    \begin{minipage}{.45\linewidth}
        \centering
        \begin{tikzpicture}[
            scale=1.4,
            transform shape,
            node distance=0.5cm,
            box/.style={
                draw,
                rounded corners,
                minimum width=2.5cm,
                minimum height=1cm,
                align=center,
                font=\small,
                fill=blue!10
                },
                arrow/.style={-{Stealth[length=2mm]}, thick},
                line/.style={thick}
                ]

                % Nodes
                \node[box, fill=blue!30] (input) {INPUT};
                \node[box, above=of input] (mlp1) {MLP};
                \node[box, above=of mlp1] (relu) {ReLU};
            \node[box, above=of relu] (mlp2) {MLP};
            \node[box, above=of mlp2, fill=blue!30] (output) {OUTPUT};

            % Connectors
            \draw[arrow] (input.north) -- (mlp1);
            \draw[arrow] (mlp1)        -- (relu);
            \draw[arrow] (relu)        -- (mlp2);
            \draw[arrow] (mlp2)        -- (output);

        \end{tikzpicture}
        \caption{Feed-Forward stack}
    \end{minipage}
\end{figure}

\vspace{1cm}

\noindent Finally, after implementing these layers in PyTorch, we will be able to start building the algorithm for a Deep Q-Learning agent using our own model, which should be capable of training on trading markets. The training strategy is as follows: the price assets are used as input to the encoder, which extracts features that are then passed to the decoder and the decision-making layer to produce the final action for a given input. We then compare the resulting portfolio with the initial one and assign a reward based on the evolution of the portfolio's value.

\newpage

\chapter{Week 14}
\minitoc

\newpage
\blankpage

\newpage

\subsection{Model Behavior Issues}

\indent After adding the commission fees, we noticed several issues, as shown below:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/expriment_on_commission/DeepQLearning_TSLA_2019_commission.png}
\end{figure}

\subsection{Environment Issues}

\noindent We initially suspected that the problems might come from the environment's behavior and reward design. The original reward was defined as simply returning the portfolio value (equity curve), so that the agent maximizes its final value. This produced the following reward:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/reward_portfolio.png}
    \caption{Reward based on the portfolio value}
\end{figure}

\noindent However, this reward led to issues in position selection. Another idea was to maximize the immediate action by comparing the past portfolio with the current one, encouraging the agent to increase portfolio value step by step:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/reward_portfolio_difference.png}
    \caption{Reward based on portfolio differences}
\end{figure}

\newpage

\noindent The problem with this approach is that it does not account for the overall final portfolio value. To address this, we kept the portfolio-based reward but multiplied it by the sign of the portfolio's derivative since the last decision, thereby favoring decisions aligned with short-term trends:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/reward_portfolio_times_derivate.png}
    \caption{Reward based on the portfolio value and its derivative sign}
\end{figure}

\noindent Yet, this method still ignored the final portfolio value. Instead of just multiplying, we then added the portfolio value multiplied by the derivative, combining long-term growth with short-term movement:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/reward_portfolio_times_derivate_additive.png}
    \caption{Reward combining portfolio value and its derivative}
\end{figure}

\noindent This reward performed better, but to further emphasize step-to-step differences, we applied a logarithmic transformation:

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{../img/reward_portfolio_times_derivate_additive_log.png}
    \caption{Reward with logarithmic adjustment}
\end{figure}

\section{Fixing the Issues}

\indent To systematically address the previously identified issues, we are currently designing a series of experiments to compare the performance of standard Q-Learning across the different reward formulations discussed in the previous section. The goal is to identify which reward structure provides the most stable learning process and the best trade-off between short-term decision-making and long-term portfolio growth.

\noindent Our methodology proceeds in two main stages:

\begin{enumerate}
    \item \textbf{Reward Function Evaluation.}  
    Each proposed reward function (portfolio value, portfolio differences, portfolio with derivative sign, additive combination, and logarithmic adjustment) will be tested under identical market conditions and training parameters. Performance metrics will include cumulative portfolio return, maximum drawdown, Sharpe ratio, and policy stability across training runs. This evaluation will allow us to assess whether a reward function encourages overfitting to short-term fluctuations, ignores long-term profitability, or achieves a balance between the two.
    
    \item \textbf{Hyperparameter Optimization.}  
    After identifying the most promising reward functions, we will perform systematic hyperparameter tuning to further improve model performance. For this purpose, we will use \textit{Optuna}, a state-of-the-art hyperparameter optimization framework that leverages efficient sampling and pruning strategies. Key hyperparameters to be tuned include learning rate, discount factor ($\gamma$), exploration rate ($\epsilon$) and its decay schedule, as well as network architecture parameters (e.g., number of layers and neurons in the Deep Q-Learning setting).
\end{enumerate}

\noindent By combining reward function selection with rigorous hyperparameter optimization, we aim to obtain a more reliable and generalizable reinforcement learning agent. Ultimately, this approach should reduce the instability observed when transaction costs are introduced and lead to a policy that remains robust under realistic market conditions.

\newpage

\chapter{Week 15}
\minitoc

\newpage
\blankpage

\newpage

\section{The Best Reward}

\indent Last week, we investigated which reward function would be most effective for our Q-Learning model in learning the dynamics of a given market. To evaluate the best reward, we developed a script that simultaneously launches 100 training sessions, allowing us to gather statistical measures for comparing each reward. All training sessions are conducted on the same dataset.

\noindent Since one batch of 100 training sessions takes an entire day to complete, we restricted our experiments to an upward trending market : the Apple stock market from 2010 to 2024. Each batch will be compared using three main criteria : the average equity curve output, the reward function output and the distribution of final profits. These metrics will help assess the robustness of each reward function.

\noindent In addition, we will analyze the maximum drawdown observed during training. To achieve this, we will compare the drawdown estimates and their distribution across all training batches.

\subsection{Reward on Raw Portfolio Returns}

\indent The "Raw Portfolio Return" reward is defined as the direct change in the portfolio value over time. In this setup, the reward signal is proportional to the basic evolution of the equity curve, without any normalization or transformation. This approach provides the most straightforward way to measure performance, as the agent's decisions are directly linked to increases or decreases in portfolio value.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_portfolio.png}
    \caption{Reward based on the portfolio value.}
\end{figure}

\noindent By construction, this type of reward strongly reflects the underlying dynamics of the market. If the agent is able to capture upward trends and avoid downturns, the reward signal will naturally increase, reinforcing profitable strategies. Conversely, poor decisions that reduce the portfolio value will be immediately penalized.

\noindent One expected advantage of this reward is that it encourages the agent to directly maximize final profit, as the cumulative reward is aligned with absolute portfolio growth. However, this alignment also comes with limitations. Since the reward is unscaled, extreme variations in portfolio value may dominate the training signal, potentially leading to instability or overfitting. Additionally, because the reward does not explicitly account for risk-adjusted performance, the agent may adopt overly aggressive strategies that expose the portfolio to large drawdowns.

\noindent In summary, the raw portfolio return reward is intuitive and easy to implement, making it a natural baseline for comparison. It emphasizes pure profitability and offers a clear benchmark against which more sophisticated reward functions can be evaluated, particularly those designed to balance profit with risk management.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/APPL_reward_portefolio.png}
    \caption{Analysis of the portfolio value reward.}
\end{figure}

\noindent The results indicate that using the raw portfolio value as a reward leads to a model with inconsistent performance. The profit distribution is centered around zero, suggesting that in most cases the agent's probability of generating profit is close to random (approximately 50\%). Such behavior is undesirable for a trading system, as it implies that the model is not consistently learning profitable strategies.

\noindent In terms of risk, the analysis reveals a maximum drawdown of $-63\%$, while the average drawdown remains close to zero as confirmed by the distribution plot. This pattern suggests that although many training runs do not experience large losses, when drawdowns occur ($-18\%$), they can be severe and catastrophic for the portfolio.

\noindent One of the key issues with this type of reward is that it provides the model with a very limited learning signal. Since the reward is tied only to the absolute portfolio value at each step, the agent struggles to evaluate the true contribution of individual actions within the broader market dynamics. In other words, the model cannot easily distinguish whether a specific action was beneficial or harmful in the long run, especially when short-term fluctuations dominate the reward signal.

\noindent To address this limitation, it would be necessary to design reward functions that place greater emphasis on the relative impact of actions, for example by considering both past performance and expected future outcomes. Overall, while the raw portfolio reward offers a direct link to profitability, its lack of sensitivity to action-level contributions and its exposure to extreme drawdowns make it an unreliable choice for robust trading strategies.


\subsection{Reward on Portfolio Differences}

\indent From the previous experiment, we observed that the raw portfolio value as a reward does not adequately represent the impact of individual actions. To better capture this impact, we introduce the "Portfolio Difference" reward. This reward is defined as the change in portfolio value between two consecutive steps ; the difference between the portfolio value after the current action and that after the previous action.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_portfolio_difference.png}
    \caption{Reward based on portfolio differences.}
\end{figure}

\noindent By construction, this reward function places greater emphasis on the immediate consequence of a decision. A profitable action will yield a positive reward, while a poor action will be penalized instantly. This local sensitivity is expected to increase the exploitation capacity of the learning agent, as it provides clearer feedback on which decisions are beneficial at each time step.

\[
    \mathcal{R} = x_{portfolio}[t] - x_{portfolio}[t-1]
\]

\noindent However, the portfolio difference reward also introduces certain limitations. Since it focuses only on the relative change from one step to the next, it does not directly optimize for long-term profitability. In other words, the agent may learn to favor short-term improvements in portfolio value without necessarily maximizing the final cumulative profit. Furthermore, this reward function may encourage reactive rather than strategic behavior, as the agent optimizes primarily with respect to the most recent outcome rather than considering broader market dynamics.

\noindent Despite these drawbacks, the portfolio difference reward offers a valuable intermediate step in reward design. It provides sharper feedback for the agent's actions compared to raw portfolio value, making it easier to distinguish between good and bad decisions in the short run. This reward can therefore serve as a useful benchmark, especially when combined or compared with more sophisticated functions that explicitly incorporate risk management and long-term profit objectives.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/APPL_reward_portefolio_diff.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\noindent The results show that the mean equity curve is improved compared to the raw portfolio reward, with fewer portfolios ending below $0\%$ profit, let us notice that the average equity curve is always above $0\%$. This improvement is also reflected in the profit distribution, which centers around $25\%$, suggesting that the portfolio difference reward produces more robust outcomes than the previous approach.  

\noindent In terms of risk, the average drawdown remains relatively similar and stable, indicating that the strategy is not yet explicitly accounting for risk management. Nevertheless, we observe a significant improvement in the maximum drawdown, which decreases to $-25.06\%$. This indicates that while large losses can still occur, their severity has been substantially reduced compared to the raw portfolio reward.

\noindent However, as noted earlier, this reward design does not explicitly maximize the final portfolio value. Instead, it emphasizes the relative impact of each action with respect to the previous step. While this provides clearer short-term learning signals, it may limit the agent's ability to optimize for long-term growth and overall profitability.

\noindent These findings suggest that while the portfolio difference reward improves robustness and reduces extreme losses, it remains incomplete as a standalone solution. A potential improvement would be to design a hybrid reward that combines short-term action sensitivity with long-term profitability objectives. Such a function could better capture the dynamic relationship between individual decisions and the final portfolio value, thereby aligning immediate learning signals with strategic performance goals.

\subsection{Reward on Slope Sign}

\indent Building on the previous reward functions, we introduce the "Slope Sign" reward, which attempts to combine the benefits of both raw portfolio returns and portfolio differences. The idea is to evaluate the direction of the equity curve by computing the slope between the portfolio value after the previous action and the current portfolio value. The sign of this slope (positive or negative) is then multiplied by the current portfolio value to generate the reward signal. In this way, profitable actions are reinforced, while unprofitable ones are directly penalized.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_portfolio_times_derivate.png}
    \caption{Reward based on the portfolio value and its derivative sign}
\end{figure}

\noindent The main advantage of this reward function is that it provides clear and immediate feedback regarding the quality of an action. Good decisions that align with upward movements in the equity curve are strongly rewarded, while poor decisions are heavily punished. This sharp distinction creates a larger gap between successful and unsuccessful actions, which can accelerate the learning process and improve the agent's ability to exploit profitable opportunities.

\[
    \mathcal{R} = \text{sign}\left(\frac{\delta}{\delta t} \left(x_{portfolio}[t_{\text{last action}}] - x_{portefolio}[t]\right)\right) * x_{portfolio}[t]
\]

\noindent However, this approach also comes with limitations. Since the slope sign is highly sensitive to short-term fluctuations, the reward signal can be noisy and unstable. Market oscillations that do not reflect meaningful long-term trends may mislead the agent, causing it to overreact to minor changes rather than learning robust strategies. In addition, because this reward focuses primarily on the direction of the immediate slope, it may neglect broader portfolio dynamics such as cumulative profit growth or drawdown control.

\noindent In summary, the slope sign reward introduces a more decisive feedback mechanism compared to previous designs, enhancing the distinction between good and bad actions. Nevertheless, its sensitivity to market oscillations makes it less reliable in volatile environments.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/APPL_reward_portefolio_sign.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\noindent This type of reward performs poorly. The likely reason is the high frequency of changes in the portfolio, which results in excessive volatility. Consequently, we observe a high drawdown and negative profits across nearly all trials. Due to these unfavorable outcomes, this reward function will not be used in subsequent analyses.

\subsection{Reward on Portfolio Value and Direct Slope}

\indent To reduce the instability introduced by using only the slope sign, we extend the reward definition by incorporating the actual slope value. Specifically, the reward is computed as the sum of the current portfolio value and the slope of the equity curve, scaled by the portfolio value at each step. This formulation preserves the influence of market trends while reducing the sensitivity to minor oscillations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_portfolio_times_derivate_additive.png}
    \caption{Reward combining portfolio value and its derivative.}
\end{figure}

\noindent With this design, the reward function captures both profitability and directional momentum. On one hand, the portfolio value ensures that the agent remains focused on long-term growth. On the other hand, the slope provides immediate feedback on whether the most recent action is aligned with the current market trend. Wrong decisions are punished proportionally to the negative slope, while good decisions are reinforced when the trend is favorable.

\[
    \mathcal{R} = \frac{\delta}{\delta t} \left(x_{portfolio}[t_{\text{last action}}] - x_{portefolio}[t]\right) * x_{portfolio}[t]
\]

\noindent The intuition behind this approach is inspired by human-like reasoning in trading : investors do not only look at their total portfolio value but also assess whether their most recent actions are still performing well relative to the ongoing market movement. By combining both perspectives, this reward function aims to balance long-term profit maximization with short-term action validation.

\noindent However, this method is not without limitations. The slope remains sensitive to volatility, and sudden short-term fluctuations can still distort the reward signal. Additionally, because the reward is scaled by the portfolio value, large portfolios may produce exaggerated feedback compared to smaller ones, potentially leading to unstable learning dynamics. A possible refinement would be to normalize the slope contribution or apply smoothing techniques to better filter out noise from transient oscillations.  

\noindent In summary, the portfolio value and direct slope reward offers a more balanced feedback mechanism than previous formulations, combining overall profitability with immediate trend sensitivity. It provides a promising middle ground, though further adjustments may be required to improve stability in highly volatile markets.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/APPL_reward_portefolio_slope.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\noindent As we can see, the profit is often positive ; however, there are also significant periods of negative returns. This suggests that the reward function is not as robust as initially expected when compared to the second approach. Moreover, the maximum drawdown is particularly severe ($-87\%$), which is considerably higher than what we typically observed in previous experiments. Finally, the strategy tends to converge to a profit level close to $50\%$ of the original wallet value.


\subsection{Reward on Portfolio Value and Direct Slope (Logarithmic Function)}

\indent Having established a good balance between portfolio value and immediate action feedback, we introduce a logarithmic adjustment to further refine the reward. This modification allows us to leverage the punishment depending on the magnitude of the slope, emphasizing significant deviations while reducing sensitivity to minor fluctuations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_portfolio_times_derivate_additive_log.png}
    \caption{Reward with logarithmic adjustment.}
\end{figure}

\noindent The core idea behind applying a logarithmic function is to amplify the impact of large positive or negative slopes. Large upward movements are rewarded more strongly, encouraging the agent to capitalize on strong trends. Conversely, steep downward movements are penalized more severely, ensuring that poor decisions with major negative consequences are recognized and discouraged.

\[
    \mathcal{R} = \text{log}\left(\frac{\delta}{\delta t} \left(x_{portfolio}[t_{\text{last action}}] - x_{portefolio}[t]\right) * x_{portfolio}[t]\right)
\]

\noindent By scaling the slope contribution logarithmically, the agent receives a more nuanced learning signal. Small fluctuations, which might otherwise generate noise in the reward, have a reduced impact, while extreme events exert a proportionally greater influence. This approach helps the agent focus on meaningful actions that significantly affect portfolio growth, rather than reacting to minor market oscillations.

\noindent In summary, the logarithmic adjustment enhances the previous reward formulation by improving sensitivity to important market movements while dampening the effect of minor, potentially misleading fluctuations. This makes the reward signal more informative and robust, leading to better alignment between immediate action evaluation and long-term portfolio performance.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/APPL_reward_portefolio_slope_log.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}


\noindent This new reward yields results that are broadly similar to the previous case, but with slight improvements. In particular, the maximum drawdown is reduced to $-74\%$, and the profit distribution is more balanced, indicating greater robustness. Additionally, the proportion of positive outcomes has increased, with profits tending to stabilize around $70\%$ of the original wallet value.

\newpage

\chapter{Week 16}
\minitoc

\newpage
\blankpage

\newpage

\section{VIX index experiments}

\indent This week we focused our agent to work on the VIX index. The \textbf{VIX Index}, also known as the \textit{CBOE Volatility Index}, is a widely followed measure of market expectations of near-term volatility conveyed by S\&P 500 stock index option prices. Introduced by the Chicago Board Options Exchange (CBOE) in 1993, the VIX is often referred to as the "fear gauge" because it tends to rise when markets become uncertain or experience stress.

\noindent Mathematically, the VIX represents the market's expectation of the annualized volatility over the next 30 days, derived from a wide range of S\&P 500 index options. Higher VIX values indicate greater expected market volatility, while lower values suggest calmer market conditions. 

\noindent investors and risk managers use the VIX as a tool for hedging, portfolio allocation, and to gauge market sentiment. It is widely used in both academic research and practical trading strategies to understand and anticipate market risk.

\subsection{Finding the best reward for VIX index}

\indent As we did previously, we need to find the best reward for the VIX index. We will keep the same kind of reward than previously and run a 100 curves for each reward to get the best possible reward.

\noindent \textbf{Portfolio Returns Reward}

\noindent This reward is based on the return of the porfolio which is suppose to maximize the final value of the equity curves associated to the trained agent.

\[
    \mathcal{R} = x_{portfolio}
\]

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/VIX_reward_portefolio.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\noindent This reward will not be considered as it has inly negative profit.

\newpage

\hfill

\noindent \textbf{Portfolio Difference Reward}

\hfill

\[
    \mathcal{R} = x_{portfolio}[t] - x_{portfolio}[t-1]
\]

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/VIX_reward_portefolio_diff.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\hfill

\noindent This reward is a good candidate since it has the best trained agent profit that reached almost $300\%$. Also, the drawdown is very low in the distribution graph.

\hfill

\noindent \textbf{Slope Sign Reward}

\hfill

\[
    \mathcal{R} = \text{sign}\left(\frac{\delta}{\delta t} \left(x_{portfolio}[t_{\text{last action}}] - x_{portefolio}[t]\right)\right) * x_{portfolio}[t]
\]

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/VIX_reward_portefolio_sign.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\hfill

\noindent The slope sign reward will not be considered since it has to many negative profit.

\hfill

\newpage

\noindent \textbf{Portfolio Value and Direct Slope Reward}

\[
    \mathcal{R} = \frac{\delta}{\delta t} \left(x_{portfolio}[t_{\text{last action}}] - x_{portefolio}[t]\right) * x_{portfolio}[t]
\]

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/VIX_reward_portefolio_slope.png}
    \caption{Analysis of the portfolio value reward}
\end{figure}

\noindent This reward has an excellent drawdown compared to the last good candidates but the best agent is only reaching $200\%$ of profit which is worse than the previous candidates.

\indent As reward we will use the portfolio difference reward since it has the best profit output and drawdown, which is not the best out of all agents, that is good enough to be considered in real life scenarios.

\newpage

\subsection{Fine-Tuning Q-Learning for the VIX Index}

\indent Now we found the best reward, we need to fine-tune the Q-Learning agent to optimize the Q-matrix outputs for making decisions on a testing dataset. To achieve this, we perform approximately 120 trials of the agent in the same environment in order to identify the best set of hyperparameters across all trials.

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/optuna_trials.png}
    \caption{Optuna output for trials}
\end{figure}

\hfill

\noindent As observed, the best trial reached a profit exceeding 300\%, indicating that the corresponding parameters are capable of training an agent with strong performance.

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/reward_analysis/optuna_parameters.png}
    \caption{Optuna parameter optimization results}
\end{figure}

\hfill

\newpage

\hfill

\noindent However, due to the stochastic nature of the greedy policy, agents trained with the same parameters may produce different returns. This means that the 300\% profit is not guaranteed in subsequent training runs. To address this variability, we train a set of 100 agents to compare their performance and select the best among them.

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.8\textwidth]{../img/reward_analysis/optuna_best_param_trials.png}
    \caption{Performance distribution of agents trained with the best parameters}
\end{figure}

\hfill

\noindent From the figure, we observe a stable drawdown distribution around 5\% for most trainings, with an average maximum drawdown of $-24\%$. The profit distribution centers around 25\%, which is favorable compared to previous market analyses. Importantly, the best trial out of the 100 trained agents achieved nearly 200\% profit.

\hfill

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.7\textwidth]{../img/reward_analysis/optuna_best_training_trials.png}
    \caption{Best performing agent from the 100 training trials}
\end{figure}

\noindent The best trial achieved a training profit of 164\% over 8 years, corresponding to an annualized return of approximately 20\%, and a tested profit of 10\% over 2 years. Observations indicate that the agent captures significant profits while avoiding major market downturns. We can now proceed to train a Deep Q-Learning agent, replacing the basic greedy strategy, to further enhance model decision-making.

\section{Deep Q-Learning Integration}

\indent In previous work, we introduced two Deep Q-Learning agents. The first uses a GPT-based transformer model sourced from Hugging Face, while the second leverages custom model layers designed according to research on constructing models specifically tailored to market actions.

\noindent The GPT transformer model had already achieved promising results but exhibited unpredictable and unstable behavior. We addressed this issue by refining the environment and preventing illegal actions during training, which significantly improved stability. The "homemade" agent incorporates multi-head attention layers and has been developed, although it has not yet been tested. We were, however, able to evaluate the impact of the new reward function and parameter settings on the GPT transformer agent :

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.9\textwidth]{../img/GPT_transformer_results/DeepQLearning_VIX_2015.png}
    \caption{Deep Q-Learning Agent (non optimized) results}
\end{figure}

\noindent This agent is limiting the drawdown by itself since it has a very stable uptrend equity curve. We have global profit that is slightly better than the previous basic Q-Learning agent by $1\%$ with a global profit (Training + Testing dataset) of $175\%$.

\begin{center}
    \begin{tabular}{l@{\hskip 1cm}|c|c|}
            &  &                                                  \\
            & \textbf{Q-Learning} & \textbf{Deep Q-Learning}      \\
            & & \textbf{(not optimized)}                          \\[0.4cm] \hline
            & &                                                   \\[-0.2cm]
            \textit{Agent Training Profit} & $163\%$ & $151\%$    \\[0.2cm]
            \textit{Agent Testing Profit} & $10\%$ & $24\%$       \\[0.5cm] \hdashline
            & &                                                   \\[-0.2cm]
            \textbf{\textit{Global Profit}} & \textbf{$173\%$} & \textbf{$175\%$} \\[0.2cm]
    \end{tabular}
\end{center}

\noindent Let's noticed that this Deep Q-Learning agent has not been optimized since that for 500 epsiodes training we have to sepdn 40 minutes and the training script needs to be optimized since it is using a lot of RAM. After fixing the RAM issue (if it is possible), we will be able to train multiple agent in the same time to fine-tune the model using optuna. For now the new agent has the best output from every model we used previously, but we suggest that the agent could get better performance after optimization.wallet value.

\newpage

\chapter{Week 17}
\minitoc

\newpage
\blankpage

\newpage

\section{Encoder Transformer}


\begin{figure}[!ht]
    \centering
    \begin{tikzpicture}[
        scale=0.9,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
        },
        arrow/.style={
        -{Stealth[length=2mm]},
        thick
        }
        ]

        % Nodes
        \node[box, fill=blue!30] (input) {INPUT};
        \node[box, right=of input, scale=0.8] (embedding) {Embedding};
        \node[box, right=of embedding, fill=gray, scale=1.4] (encoder) {Encoder};
        \node[box, right=of encoder, fill=gray, scale=1.4] (decoder) {Decoder};
        \node[box, right=of decoder, scale=0.8] (normalization) {Normalization};
        \node[box, right=of normalization, fill=blue!30] (out) {OUTPUT};

        % Connectors and Labels
        \draw[arrow] (input) -- (embedding);
        \draw[arrow] (embedding) -- (encoder);
        \draw[arrow] (encoder) -- (decoder);
        \draw[arrow] (decoder) -- (normalization);
        \draw[arrow] (normalization) -- (out);

    \end{tikzpicture}
    \caption{Encoder-Decoder Stack of the transformer model}
\end{figure}

\indent Few weeks ago, we read the paper written in 2023 by Pranav Agarwal (cole de Technologie Suprieure/ Mila, Canada), Aamer Abdul Rahman (cole de Technologie Suprieure/Mila, Canada), Pierre-Luc St-Charles (Mila, Applied ML Research Team, Canada), Simon J.D. Prince (University of Bath, United Kingdom) and Samira Ebrahimi Kahou (cole de Technologie Suprieure/Mila/CIFAR, Canada), provides a comprehensive overview of this rapidly developing research area. The paper reviews how transformers have been adapted for reinforcement learning, highlights emerging architectures and methodologies, and categorizes them according to their applications. It also discusses empirical results, key challenges such as scalability and sample efficiency, and potential future research directions.\par

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[
        scale=1,
        transform shape,
        node distance=0.5cm,
        box/.style={
            draw,
            rounded corners,
            minimum width=2.5cm,
            minimum height=1cm,
            align=center,
            font=\small,
            fill=blue!10
        },
        arrow/.style={-{Stealth[length=2mm]}, thick},
        line/.style={thick}
        ]

    % Nodes
    \node[box, fill=blue!30] (input) {INPUT};
    \node[coordinate, above=of input] (junction1) {} ;
    \node[box, above=of junction1,fill=gray ,scale=1.4] (mha) {Multi-Head\\ Attention};
    \node[box, above=of mha, scale=0.9] (AddNorm1) {Add \& Norm};
    \node[coordinate, above=of AddNorm1] (junction2) {} ;
    \node[box, above=of junction2] (ff) {Feed Forward} ;
    \node[box, above=of ff] (AddNorm2) {Add \& Norm} ;
    \node[box, above=of AddNorm2, fill=blue!30] (output) {OUTPUT};

    % Connectors
    \draw[line] (input.north) -- (junction1);
    \draw[arrow] (junction1) -- (mha);
    \draw[arrow] (mha) -- (AddNorm1);
    \draw[arrow] (junction1.west) -- ++(-2.5, 0) |- (AddNorm1.west);
    \draw[arrow] (AddNorm1) -- (junction2);
    \draw[line] (junction2) -- (ff);
    \draw[arrow] (ff) -- (AddNorm2);
    \draw[arrow] (junction2.west) -- ++(-2.5, 0) |- (AddNorm2.west);
    \draw[arrow] (AddNorm2) -- (output);

\end{tikzpicture}
\caption{Encoder stack}
\vspace{-.8cm}
\end{wrapfigure}

\noindent By situating transformer-based reinforcement learning within the broader landscape of sequence modeling and decision-making, this survey aims to deepen our understanding of both the opportunities and limitations of applying transformers in RL contexts. It serves as a reference point for researchers and practitioners seeking to build on the foundations of this promising intersection of fields. While transformers have demonstrated remarkable success across natural language processing, computer vision, and sequential prediction tasks, their application to financial trading is particularly compelling, given the complex, dynamic and highly stochastic nature of financial markets.

\noindent In the context of trading, the model architecture can be divided into two core components. The encoder, built upon a multi-head attention mechanism, is responsible for capturing the long-term dependencies and intricate temporal patterns in financial time-series data. This design enables the model to represent market dynamics more effectively than traditional recurrent architectures, which often struggle with long-horizon dependencies. The decoder, on the other hand, is structured with varying output dimensions tailored to the specific prediction or decision-making task at hand, such as making an action.

\indent By combining these components, the transformer-based RL framework is capable of not only learning sequential representations of market states but also optimizing decision policies that directly impact trading performance. This dual capability provides an advantage over conventional predictive models, which typically focus only on forecasting without explicitly linking predictions to downstream decision-making. Moreover, reinforcement learning introduces a feedback loop that allows the agent to continuously adapt its strategy in response to evolving market conditions, thereby improving its robustness and generalization ability.

\newpage

\section{First Encoder Transformer results}

\indent We performed 100 trials with Optuna to compare the best-performing agents. These trials optimized the hyperparameters of the reinforcement learning framework, including the learning rate, batch size, discount factor, and replay buffer capacity. By systematically searching through this space, Optuna was able to identify combinations that improved both convergence speed and trading performance.  

\noindent The training setup, as illustrated in our implementation, involved running the agent for 500 episodes with a training size of 80\% of the dataset. A smaller learning rate ($1\times 10^{-4}$) was chosen to ensure stability during optimization, while the discount factor ($\gamma = 0.99$) preserved the importance of long-term rewards. To encourage exploration in the early stages, the policy started with a maximum epsilon value of 1.0, gradually decaying to 0.05 with a rate of 0.005. This ensured that the agent initially explored the market environment widely before converging toward exploitation of profitable strategies.

\noindent Additionally, a large replay buffer of 100,000 transitions was used to stabilize learning by diversifying the sampled experiences, while a batch size of 128 allowed for more robust gradient updates. The target network was updated every 500 steps, and learning occurred every 4 steps to maintain a balance between computational efficiency and training stability. A warm-up phase of 5,000 random steps was also introduced to populate the replay buffer before policy learning began.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.7\textwidth]{../img/DQL_profit/optuna_best_encoder_transformer.png}
    \caption{Encoder Transformer Best optuna trials}
\end{figure}

\noindent This configuration provided a strong baseline for comparing the agents optimized by Optuna. The results of these experiments highlighted which hyperparameter settings led to consistent profitability, reduced volatility in returns, and improved sample efficiency. The following section presents a detailed analysis of the performance metrics obtained from these trials, including cumulative returns, Sharpe ratio and maximum drawdown.

\indent Finally, we found the best trail to have more than $5\%$ of profit while previously we had a profit of $10\%$. The basic previous nodel seems to be better. However let's run 100 trials with the optimized hyperparameterization to get a better understanding of the best agent. For example, there could be a drawdown anomaly with more or less frequency of actions taken.

\newpage

\subsection{Getting the best DeepQLearning agent}

\indent Out of a 100 trials, we found the best agent to make $5\%$ of profit, this is two times less than the previous basic QLearning agent. However, the drawdis especially low with a Maximum Drawdown of $3\%$ which is very low. Also, the drawdown distribution is even lower with an average distribution lower than $1.5\%$.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{../img/DQL_profit/best_encoder_transformer.png}
    \caption{Bests Encoder Transformer}
\end{figure}

\noindent As we can see, the agent appears to capture the underlying market dynamics effectively during the training phase. When evaluated on the test set, the model demonstrates an ability to identify and exploit the main market peaks, thereby securing the majority of potential profits. At the same time, it maintains a relatively low drawdown, which is a critical factor in assessing the reliability of trading strategies. This balance between profit generation and risk management suggests that the agent is not only capable of learning profitable patterns but also of adopting a conservative behavior that mitigates excessive losses.  

\noindent While this conservative approach may limit the overall profit potential compared to highly aggressive strategies, it contributes to the robustness and stability of the trading policy. In practice, such behavior is often more desirable, as it prioritizes capital preservation and consistency over occasional large gains accompanied by higher risk. These results therefore highlight the suitability of transformer-based reinforcement learning agents for financial applications where reliability and risk control are of paramount importance.  


\begin{figure}[!ht]
    \centering
    \includegraphics[width=.7\textwidth]{../img/DQL_profit/homemade_DQL_profit.png}
    \caption{Encoder Transformer Results}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.7\textwidth]{../img/DQL_profit/homemade_DQL_profit_closeup.png}
    \caption{Encoder Transformer Results (test focus)}
\end{figure}

%\newpage

%\appendix


%\newpage

%Rfrences
%\printbibliography

\newpage

%Glossaire
\printacronyms

% \newpage

% \printglossary

% %Annexes
% \section{Annexes}
% \input{annexe.tex}

\end{document}
